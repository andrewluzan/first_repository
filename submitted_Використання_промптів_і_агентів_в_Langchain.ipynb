{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMalK_oYR-X_"
   },
   "source": [
    "\n",
    "### –ó–∞–≤–¥–∞–Ω–Ω—è 1: –í–∏–∫–ª–∏–∫ LLM –∑ –±–∞–∑–æ–≤–∏–º –ø—Ä–æ–º–ø—Ç–æ–º\n",
    "\n",
    "–°—Ç–≤–æ—Ä—ñ—Ç—å –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –≤–∏–∫–ª–∏–∫–∞—Ç–∏ LLM –∑—ñ –∑–≤–∏—á–∞–π–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–≤–∏–º –ø—Ä–æ–º–ø—Ç–æ–º.\n",
    "\n",
    "–ü—Ä–æ–º–ø—Ç –º–∞—î –¥–æ–∑–≤–ª—è—Ç–∏ –æ—Ç—Ä–∏–º–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ—Å—Ç–æ—é –º–æ–≤–æ—é –Ω–∞ –ø–µ–≤–Ω—É —Ç–µ–º—É. –í —Ü—å–æ–º—É –∑–∞–≤–¥–∞–Ω–Ω—ñ –º–∏ —Ö–æ—á–µ–º–æ –¥—ñ–∑–Ω–∞—Ç–∏—Å—å –ø—Ä–æ —Ç–µ–º—É \"–ö–≤–∞–Ω—Ç–æ–≤—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è\".\n",
    "\n",
    "–í—ñ–¥–ø–æ–≤—ñ–¥—å –º–æ–¥–µ–ª—ñ –ø–æ–≤–∏–Ω–Ω–∞ –º—ñ—Å—Ç–∏—Ç–∏ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è, –∫–ª—é—á–æ–≤—ñ –ø–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –ø–æ—Ç–æ—á–Ω—ñ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –≤ —Ü—ñ–π –≥–∞–ª—É–∑—ñ.\n",
    "\n",
    "–û–±–º–µ–∂—Ç–µ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –¥–æ 200 —Å–∏–º–≤–æ–ª—ñ–≤ —ñ –ø—Ä–æ–ø–∏—à—ñ—Ç—å –≤ –ø—Ä–æ–º–ø—Ç—ñ –∞–±–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –±—É–ª–∞ –∫–æ—Ä–æ—Ç–∫–æ—é (—Ü–µ –∑–µ–∫–æ–Ω–æ–º–∏—Ç—å –í–∞–º —á–∞—Å —ñ –≥—Ä–æ—à—ñ –Ω–∞ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ —Ç–æ–∫–µ–Ω–∏).\n",
    "\n",
    "–í —è–∫–æ—Å—Ç—ñ LLM –º–æ–∂–Ω–∞ —Å–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏—Å—å —è–∫ –º–æ–¥–µ–ª–ª—é –∑ HugginFace (—Ä–µ–∫–æ–º–µ–Ω–¥—É—é Mistral), —Ç–∞–∫ —ñ ChatGPT4 –∞–±–æ ChatGPT3. –í –æ–±–æ—Ö –≤–∏–ø–∞–¥–∫–∞—Ö —Ç—Ä–µ–±–∞ —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –ø–æ—Ç—Ä—ñ–±–Ω—É \"–æ–±–≥–æ—Ä—Ç–∫—É\" (—Ç–æ–±—Ç–æ –∫–ª–∞—Å, —è–∫–∏–π –¥–æ–∑–≤–æ–ª–∏—Ç—å —ñ–Ω—ñ—Ü—ñ—é–≤–∞—Ç–∏ –º–æ–¥–µ–ª—å) –∑ LangChain –¥–ª—è –≤–∏–∫–ª–∏–∫—É LLM –∑–∞ API, –∞ —Ç–∞–∫–æ–∂ –∑—á–∏—Ç–∞—Ç–∏ –æ—Å–æ–±–∏—Å—Ç–∏–π —Ç–æ–∫–µ–Ω –∑ —Ñ–∞–π–ª–∞, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, `creds.json`, —è–∫–∏–π —Ä–æ–∑–º—ñ—â–µ–Ω–∏–π —É –í–∞—Å –ª–æ–∫–∞–ª—å–Ω–æ —ñ –í–∏ –ù–ï –∑–¥–∞—î—Ç–µ –π–æ–≥–æ –≤ –î–ó —ñ –ù–ï –∫–æ–º—ñ—Ç–∏—Ç–µ –≤ git üòè\n",
    "\n",
    "–í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å —Å–≤–æ—î –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∏ –Ω–∞ —Å–≤—ñ–π —Ä–æ–∑—Å—É–¥ (—Ç—É—Ç –Ω–µ–º–∞—î –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —á–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è) —ñ –Ω–∞–ø–∏—à—ñ—Ç—å, —á–æ–º—É –≤–∏ –æ–±—Ä–∞–ª–∏ —Å–∞–º–µ —Ç–∞–∫–µ –∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è —Ü—å–æ–≥–æ –∑–∞–≤–¥–∞–Ω–Ω—è.  \n",
    "\n",
    "–ó–∞–ø–∏—Ç–∏ –º–æ–∂–Ω–∞ —Ä–æ–±–∏—Ç–∏ —è–∫ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é, —Ç–∞–∫ —ñ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é - –æ—Ä—ñ—î–Ω—Ç—É–π—Ç–µ—Å—å –Ω–∞ —Ç–µ, –¥–µ —ñ —á–∏ —Ö–æ—á–µ—Ç–µ –≤–∏ –ø–æ—Ç—ñ–º –ª–∏—à–∏—Ç–∏ —Ü–µ–π –ø—Ä–æ—î–∫—Ç —ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∞ —è–∫–∞ –º–æ–≤–∞ –±—É–¥–µ –ø–∞—Å—É–≤–∞—Ç–∏ –±—ñ–ª—å—à–µ. –í —Ä–æ–∑–≤ º—è–∑–∫–∞—Ö –ø—Ä–æ–º–ø—Ç–∏ - —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vT1ICe74cAUS"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open('creds.json') as file:\n",
    "  creds = json.load(file)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = creds[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ 0.3 ‚Äì –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–∏—Ö —Ç–∞ –ª–∞–∫–æ–Ω—ñ—á–Ω–∏—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π. –ü—ñ–¥—ñ–π–¥–µ –¥–ª—è —Ü—å–æ–≥–æ –∑–∞–≤–¥–∞–Ω–Ω—è, –æ—Å–∫—ñ–ª—å–∫–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ –æ—Ç—Ä–∏–º–∞—Ç–∏ —á—ñ—Ç–∫—É, —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é.\n",
    "overal_temperature = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=overal_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "–ö–≤–∞–Ω—Ç–æ–≤—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è - —Ü–µ –º–µ—Ç–æ–¥ –æ–±—Ä–æ–±–∫–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó, —è–∫–∏–π –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –∫–≤–∞–Ω—Ç–æ–≤—ñ —Å–∏—Å—Ç–µ–º–∏ –∑–∞–º—ñ—Å—Ç—å –∫–ª–∞—Å–∏—á–Ω–∏—Ö –±—ñ—Ç—ñ–≤. –í–æ–Ω–∏ –¥–æ–∑–≤–æ–ª—è—é—Ç—å –≤–∏–∫–æ–Ω—É–≤–∞—Ç–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è —à–≤–∏–¥—à–µ —Ç–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—à–µ, –∑–æ–∫—Ä–µ–º–∞ –¥–ª—è –∑–∞–¥–∞—á, —è–∫—ñ –Ω–µ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –≤–∏—Ä—ñ—à–µ–Ω—ñ –∫–ª–∞—Å–∏—á–Ω–∏–º–∏ –∫–æ–º–ø'—é—Ç–µ—Ä–∞–º–∏. –ü–æ—Ç–æ—á–Ω—ñ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è —Å–ø—Ä—è–º–æ–≤–∞–Ω—ñ –Ω–∞ —Ä–æ–∑—Ä–æ–±–∫—É –∫–≤–∞–Ω—Ç–æ–≤–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤ —Ç–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –±—ñ–ª—å—à –ø–æ—Ç—É–∂–Ω–∏—Ö —Ç–∞ —Å—Ç–∞–±—ñ–ª—å–Ω–∏—Ö –∫–≤–∞–Ω—Ç–æ–≤–∏—Ö –∫–æ–º–ø'—é—Ç–µ—Ä—ñ–≤.\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–æ–º–ø—Ç –¥–ª—è –≤–∏–∫–ª–∏–∫—É –º–æ–¥–µ–ª—ñ\n",
    "prompt = \"–ü–æ—è—Å–Ω–∏, —â–æ —Ç–∞–∫–µ –∫–≤–∞–Ω—Ç–æ–≤—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è, –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è, —ó—Ö –ø–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –ø–æ—Ç–æ—á–Ω—ñ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è. –í—ñ–¥–ø–æ–≤—ñ–¥—å –º–∞—î –±—É—Ç–∏ –¥—É–∂–µ –∫–æ—Ä–æ—Ç–∫–æ—é, –Ω–µ –±—ñ–ª—å—à–µ 200 —Å–∏–º–≤–æ–ª—ñ–≤.\"\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ\n",
    "response = llm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiIzV0UIS0GA"
   },
   "source": [
    "### –ó–∞–≤–¥–∞–Ω–Ω—è 2: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É\n",
    "–¢–µ–ø–µ—Ä –º–∏ —Ö–æ—á–µ–º–æ –æ–Ω–æ–≤–∏—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Ñ—É–∫–Ω—Ü—ñ–æ–Ω–∞–ª —Ç–∞–∫, –∞–±–∏ –≤ –ø—Ä–æ–º–ø—Ç –º–∏ –º–æ–≥–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç–∏ —Ç–µ–º—É —è–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä. –î–ª—è —Ü—å–æ–≥–æ —Å–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ—Å—å `PromptTemplate` –∑ `langchain` —ñ —Ä–µ–∞–ª—ñ–∑—É–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–∏–π –ø—Ä–æ–º–ø—Ç —Ç–∞ –≤–∏–∫–ª–∏–∫ –º–æ–¥–µ–ª—ñ –∑ –Ω–∏–º.\n",
    "\n",
    "–ó–∞–ø—É—Å—Ç—ñ—Ç—å –æ–Ω–æ–≤–ª–µ–Ω–∏–π —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª (–ø—Ä–æ–º–ø—Ç + –º–æ–¥–µ–ª) –¥–ª—è –ø–æ—è—Å–Ω–µ–Ω—å –ø—Ä–æ —Ç–µ–º–∏\n",
    "- \"–ë–∞—î—Å—ñ–≤—Å—å–∫—ñ –º–µ—Ç–æ–¥–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\"\n",
    "- \"–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\"\n",
    "- \"Explainable AI\"\n",
    "\n",
    "–í–∏–≤–µ–¥—ñ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤—ñ–¥–ø—Ä–∞—Ü—é–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞ –µ–∫—Ä–∞–Ω."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kcg7EfdNcBDq"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"Theme\"],\n",
    "    template=\"–ü–æ—è—Å–Ω–∏, —â–æ —Ç–∞–∫–µ {Theme}? –í—ñ–¥–ø–æ–≤—ñ–¥—å –º–∞—î –±—É—Ç–∏ –¥—É–∂–µ –∫–æ—Ä–æ—Ç–∫–æ—é, –Ω–µ –±—ñ–ª—å—à–µ 200 —Å–∏–º–≤–æ–ª—ñ–≤.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–æ—è—Å–Ω–∏, —â–æ —Ç–∞–∫–µ –ë–∞—î—Å—ñ–≤—Å—å–∫—ñ –º–µ—Ç–æ–¥–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ? –í—ñ–¥–ø–æ–≤—ñ–¥—å –º–∞—î –±—É—Ç–∏ –¥—É–∂–µ –∫–æ—Ä–æ—Ç–∫–æ—é, –Ω–µ –±—ñ–ª—å—à–µ 200 —Å–∏–º–≤–æ–ª—ñ–≤.\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(Theme=\"–ë–∞—î—Å—ñ–≤—Å—å–∫—ñ –º–µ—Ç–æ–¥–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "–ë–∞—î—Å—ñ–≤—Å—å–∫—ñ –º–µ—Ç–æ–¥–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ - —Ü–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω—ñ –º–µ—Ç–æ–¥–∏, —è–∫—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å —Ç–µ–æ—Ä–µ–º—É –ë–∞—î—Å–∞ –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –≤–∏–Ω–∏–∫–Ω–µ–Ω–Ω—è –ø–æ–¥—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞—è–≤–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó —Ç–∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö –∑–Ω–∞–Ω—å. –í–æ–Ω–∏ –¥–æ–∑–≤–æ–ª—è—é—Ç—å –≤—Ä–∞—Ö–æ–≤—É–≤–∞—Ç–∏ –Ω–µ –≤–∏–∫–ª—é—á–Ω–æ –¥–∞–Ω—ñ, –∞–ª–µ —ñ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞–Ω–Ω—è, —â–æ –∑—Ä–æ–±–∏–ª–∏ —ó—Ö –ø–æ–ø—É–ª—è—Ä–Ω–∏–º–∏ –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –æ–±–º–µ–∂–µ–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(prompt.format(Theme=\"–ë–∞—î—Å—ñ–≤—Å—å–∫—ñ –º–µ—Ç–æ–¥–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–æ—è—Å–Ω–∏, —â–æ —Ç–∞–∫–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ? –í—ñ–¥–ø–æ–≤—ñ–¥—å –º–∞—î –±—É—Ç–∏ –¥—É–∂–µ –∫–æ—Ä–æ—Ç–∫–æ—é, –Ω–µ –±—ñ–ª—å—à–µ 200 —Å–∏–º–≤–æ–ª—ñ–≤.\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(Theme=\"–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ - —Ü–µ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ–Ω–Ω–æ—ó –º–µ—Ä–µ–∂—ñ, —è–∫–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –º–µ—Ö–∞–Ω—ñ–∑–º —Å–∞–º–æ—É–≤–∞–≥–∏ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π –¥–∞–Ω–∏—Ö, —Ç–∞–∫–∏—Ö —è–∫ —Ç–µ–∫—Å—Ç –∞–±–æ –º–æ–≤–∞. –í–æ–Ω–∏ –∑–¥–∞—Ç–Ω—ñ –¥–æ—Å—è–≥–∞—Ç–∏ –≤–∏—Å–æ–∫–æ—ó —Ç–æ—á–Ω–æ—Å—Ç—ñ –≤ –∑–∞–≤–¥–∞–Ω–Ω—è—Ö –æ–±—Ä–æ–±–∫–∏ –º–æ–≤–∏ —Ç–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–∫–ª–∞–¥—É.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(prompt.format(Theme=\"–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–æ—è—Å–Ω–∏, —â–æ —Ç–∞–∫–µ Explainable AI? –í—ñ–¥–ø–æ–≤—ñ–¥—å –º–∞—î –±—É—Ç–∏ –¥—É–∂–µ –∫–æ—Ä–æ—Ç–∫–æ—é, –Ω–µ –±—ñ–ª—å—à–µ 200 —Å–∏–º–≤–æ–ª—ñ–≤.\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(Theme=\"Explainable AI\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Explainable AI (XAI) - —Ü–µ –ø—ñ–¥—Ö—ñ–¥ –¥–æ —Ä–æ–∑—Ä–æ–±–∫–∏ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–∏—Ö —Å–∏—Å—Ç–µ–º, —è–∫–∏–π –¥–æ–∑–≤–æ–ª—è—î –∑—Ä–æ–∑—É–º—ñ—Ç–∏ –ø—Ä–∏—á–∏–Ω–∏ –ø—Ä–∏–π–Ω—è—Ç–∏—Ö —Ä—ñ—à–µ–Ω—å —Ç–∞ –ø–æ—è—Å–Ω–∏—Ç–∏ —ó—Ö –ª—é–¥—è–º. –¶–µ –≤–∞–∂–ª–∏–≤–æ –¥–ª—è –∑–∞–±–µ–∑–ø–µ—á–µ–Ω–Ω—è –¥–æ–≤—ñ—Ä–∏ —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω–æ—Å—Ç—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(prompt.format(Theme=\"Explainable AI\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9UsL2gXSe-3"
   },
   "source": [
    "\n",
    "\n",
    "### –ó–∞–≤–¥–∞–Ω–Ω—è 3: –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó –ø—Ä–æ—Ü–µ—Å—ñ–≤\n",
    "–°—Ç–≤–æ—Ä—ñ—Ç—å –∞–≥–µ–Ω—Ç–∞, —è–∫–∏–π –¥–æ–ø–æ–º–æ–∂–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —à—É–∫–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –æ—Å—Ç–∞–Ω–Ω—ñ –Ω–∞—É–∫–æ–≤—ñ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó –≤ —Ä—ñ–∑–Ω–∏—Ö –≥–∞–ª—É–∑—è—Ö. –ù–∞–ø—Ä–∏–∫–ª–∞–¥, –∞–≥–µ–Ω—Ç –º–∞—î –∑–Ω–∞–π—Ç–∏ 5 –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π –Ω–∞ —Ç–µ–º—É —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É.\n",
    "\n",
    "**–ö—Ä–æ–∫–∏:**\n",
    "1. –ù–∞–ª–∞—à—Ç—É–π—Ç–µ –∞–≥–µ–Ω—Ç–∞ —Ç–∏–ø—É ReAct –≤ LangChain –¥–ª—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤.\n",
    "2. –°—Ç–≤–æ—Ä—ñ—Ç—å –ø—Ä–æ–º–ø—Ç, —è–∫–∏–π —Å–ø—Ä—è–º–æ–≤—É—î –∞–≥–µ–Ω—Ç–∞ —à—É–∫–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –≤ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—ñ –∞–±–æ –≤ –±–∞–∑–∞—Ö –¥–∞–Ω–∏—Ö –Ω–∞—É–∫–æ–≤–∏—Ö –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π.\n",
    "3. –ê–≥–µ–Ω—Ç –ø–æ–≤–∏–Ω–µ–Ω –≤–∏–¥–∞—Ç–∏ —Å–ø–∏—Å–æ–∫ –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π, –∫–æ–∂–Ω–∞ –∑ —è–∫–∏—Ö –º—ñ—Å—Ç–∏—Ç—å –Ω–∞–∑–≤—É, –∞–≤—Ç–æ—Ä—ñ–≤ —ñ –∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å.\n",
    "\n",
    "–î–ª—è –≤–∑–∞—î–º–æ–¥—ñ—ó –∑ –ø–æ—à—É–∫–æ–º —Ç–∞–º –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ `Tool`. –í –ª–µ–∫—Ü—ñ—ó –º–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–ª–∏ `serpapi`. –ú–æ–∂–Ω–∞ –ø—Ä–æ–¥–æ–≤–∂–∏—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—Ç–∏—Å—å –Ω–∏–º, –∞–±–æ –æ–±—Ä–∞—Ç–∏ —ñ–Ω—à–µ –ê–†–Ü –¥–ª—è –ø–æ—à—É–∫—É (–≤–æ–Ω–∏ –≤ —Ç–æ–º—É —á–∏—Å–ª—ñ —î –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω—ñ). –ü–µ—Ä–µ–ª—ñ–∫ —Ä—ñ–∑–Ω–∏—Ö –ê–†–Ü, –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –≤ langchain, —ñ –æ—Ä—ñ—î–Ω—Ç–∏—Ä –ø–æ –≤–∞—Ä—Ç–æ—Å—Ç—ñ –∑–∞–ø–∏—Ç—ñ–≤ –º–æ–∂–Ω–∞ –∑–Ω–∞–π—Ç–∏ –≤ –æ–∫—Ä–µ–º–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—ñ [—Ç—É—Ç](https://hannapylieva.notion.site/API-12994835849480a69b2adf2b8441cbb3?pvs=4).\n",
    "\n",
    "–õ–∏—à–∞—é —Ç–∞–∫–æ–∂ –Ω–∏–∂—á–µ –ø—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –æ–¥–Ω–æ–≥–æ –∑ –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∏—Ö –ø–æ—à—É–∫–æ–≤–∏—Ö –ê–†–Ü - DuckDuckGo (–Ω–µ –ø–æ—Ç—Ä–µ–±—É—î —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ–∫–µ–Ω–∞!)  - –º–æ–∂–ª–∏–≤–æ –≤—ñ–Ω –≤–∞–º —Å–ø–æ–¥–æ–±–∞—î—Ç—å—Å—è :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "60jZEitmcCiP",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in /Users/luzan/Library/Python/3.11/lib/python/site-packages (0.3.14)\n",
      "Requirement already satisfied: openai in /Users/luzan/Library/Python/3.11/lib/python/site-packages (1.59.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langchain) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langchain) (3.11.11)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langchain) (0.3.29)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langchain) (0.3.5)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langchain) (0.2.10)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langchain) (2.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (0.28.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (24.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.14)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.agents.react.base import ReActChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import Tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SERPAPI_API_KEY\"] = creds[\"SERPAPI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luzan/Library/Python/3.11/lib/python/site-packages/langsmith/client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tools = load_tools([\"serpapi\"], llm=llm)\n",
    "prompt = hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to search for the latest scientific publications on artificial intelligence, including their titles, authors, and a brief description. I will use the search tool to find this information.\n",
      "Action: Search\n",
      "Action Input: \"latest scientific publications on artificial intelligence\"\u001b[0m\u001b[36;1m\u001b[1;3m[' entity_type: related_questions.', \"The journal's scope encompasses all areas of AI, including agents and multi-agent systems, automated reasoning, constraint processing and search ...\", 'Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI. Danaja Rutar, Alva Markelius, Konstantinos Voudouris, Jos√© ...', \"Access leading publications on AI and big data that are shaping industry understanding and capabilities, curated by Certainly's experts.\", 'The AI Index is recognized globally as one of the most credible and authoritative sources for data and insights on artificial intelligence. Previous editions ...', 'The journal of Artificial Intelligence (AIJ) welcomes papers on broad aspects of AI that constitute advances in the overall field.', 'AI system predicts protein fragments that can bind to or inhibit a target. FragFold, developed by MIT Biology researchers, is a computational method with ...', 'AI models are growing ever-more capable, accurate and impressive. The question of if they represent ‚Äúgeneral intelligence‚Äù is increasingly moot. By Lauren ...', 'Artificial intelligence software is increasingly involved in reviewing papers, provoking interest and unease. By. Miryam Naddaf.', \"AI Needs to Be More Energy-Efficient ¬∑ Is ChatGPT a Drug? ¬∑ $0 for Digital Access ¬∑ The AI Future Is Here ¬∑ Google's AI Can Beat the Smartest High Schoolers in Math.\"]\u001b[0m\u001b[32;1m\u001b[1;3mThe search results do not provide specific information about the latest scientific publications on artificial intelligence. I need to refine my search to get more specific results.\n",
      "Action: Search\n",
      "Action Input: \"latest scientific articles on artificial intelligence 2022\"\u001b[0m\u001b[36;1m\u001b[1;3m[' entity_type: related_questions.', 'Artificial Intelligence News. Everything on AI including futuristic robots with artificial intelligence, computer models of human intelligence and more.', \"The journal's scope encompasses all areas of AI, including agents and multi-agent systems, automated reasoning, constraint processing and search ...\", 'Are You Smarter Than A.I.? Some experts predict that A.I. will surpass human intelligence within the next few years. ¬∑ How Artificial Intelligence Reasons.', 'The AI Index is recognized globally as one of the most credible and authoritative sources for data and insights on artificial intelligence. Previous editions ...', 'The journal of Artificial Intelligence (AIJ) welcomes papers on broad aspects of AI that constitute advances in the overall field.', 'Artificial Intelligence Review is a fully open access journal publishing state-of-the-art research in artificial intelligence and cognitive science.', 'In the latest survey, 78 percent of respondents say their organizations use AI in at least one business function, up from 72 percent in ...', 'This study examines the impact of artificial intelligence (AI) on loss in decision-making, laziness, and privacy concerns among university ...', 'Explores the disruptive technological revolution of AI. A nexus for research in core and applied AI areas, this journal focuses on the enormous expansion of AI ...', 'A new National Academies report recommends ways for the U.S. to reap the benefits of artificial intelligence in biotechnology while minimizing risks that AI may ...']\u001b[0m\u001b[32;1m\u001b[1;3mThe search results still do not provide specific information about the latest scientific publications on artificial intelligence. I need to refine my search to get more specific results.\n",
      "Action: Search\n",
      "Action Input: \"recent scientific papers on artificial intelligence 2022\"\u001b[0m\u001b[36;1m\u001b[1;3m[' entity_type: related_questions.', \"The journal's scope encompasses all areas of AI, including agents and multi-agent systems, automated reasoning, constraint processing and search ...\", 'A curated list of the latest breakthroughs in AI (in 2022) by release date with a clear video explanation, link to a more in-depth article, and code.', 'The journal of Artificial Intelligence (AIJ) welcomes papers on broad aspects of AI that constitute advances in the overall field.', 'Qualcomm contributes impactful artificial intelligence research covering computer vision, machine learning, generative modeling, and more.', 'Artificial Intelligence News. Everything on AI including futuristic robots with artificial intelligence, computer models of human intelligence and more.', 'Today I return to my series of AI paper recommendations bringing you ten new AI paper suggestions (and many more links).', 'Artificial Intelligence Review is a fully open access journal publishing state-of-the-art research in artificial intelligence and cognitive science.', 'Papers With Code highlights trending Machine Learning research and the code to implement it.', 'We caught up with experts in the RE‚Ä¢WORK community to find out what the top 17 AI papers are for 2022 so far that you can add to your Summer must reads.', 'We write about the progress and impact of artificial intelligence through a cross-sector lens, from research and development to technical performance, the ...']\u001b[0m\u001b[32;1m\u001b[1;3mThe search results still do not provide specific information about the latest scientific publications on artificial intelligence. I need to refine my search to get more specific results.\n",
      "Action: Search\n",
      "Action Input: \"recently published artificial intelligence research papers 2022\"\u001b[0m\u001b[36;1m\u001b[1;3m[' entity_type: related_questions.', 'A curated list of the latest breakthroughs in AI (in 2022) by release date with a clear video explanation, link to a more in-depth article, and code.', \"The journal's scope encompasses all areas of AI, including agents and multi-agent systems, automated reasoning, constraint processing and search ...\", 'Below are papers that Qualcomm AI Research has written or co-authored. ... Machine Learning Methods | Globecom 2022 ¬∑ Neural RF SLAM for unsupervised ...', 'LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning ¬∑ Cognitive Science-Inspired Evaluation of Core ...', 'Stay informed on the latest trending ML papers with code, research developments, libraries, methods, and datasets. Read previous issues. Subscribe ...', 'Our work provides valuable insights and future research directions for improving the reliability of AI supercomputer clusters, emphasizing the need for flexible ...', '2022 ; Explicit group sparse projection with applications to deep learning and NMF ¬∑ Riyasat Ohib, Nicolas Gillis, Niccolo Dalmasso, Sameena Shah, Vamsi Potluru, ...', 'This repository is an up-to-date list of significant AI papers organized by publication date. It covers five fields : computer vision, natural language ...', 'The journal of Artificial Intelligence (AIJ) welcomes papers on broad aspects of AI that constitute advances in the overall field.', 'We caught up with experts in the RE‚Ä¢WORK community to find out what the top 17 AI papers are for 2022 so far that you can add to your Summer must reads.']\u001b[0m\u001b[32;1m\u001b[1;3mThe search results now provide some specific information about the latest scientific publications on artificial intelligence. I can see some titles and authors, but I still need more detailed information to provide a brief description for each publication.\n",
      "Action: Search\n",
      "Action Input: \"LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning\"\u001b[0m\u001b[36;1m\u001b[1;3m['This study aims to develop a Gomoku AI system based on LLMs, simulating the human learning process of playing chess. The system is de-signed to ...', 'This study targets developing a Gomoku AI system using LLMs, focusing on human-like learning. The system aims to understand and apply Gomoku ...', 'LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning PAPER: https://arxiv.org/abs ...', '1. LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning ¬∑ 2. Cognitive Science-Inspired Evaluation of ...', 'Title: LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning. Hui Wang. Subjects: Artificial ...', 'LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning. Yesterday by Hui Wang ¬∑ Artificial Intelligence ...', 'LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning. RL. NLP. ‚ö° Summary (one liner).', 'This thesis aims to build an artificial intelligence for Gomoku using neural networks and reinforcement learning, resulting in a solution that ...', 'Reinforcement learning (RL) allows non-player characters (NPCs) or game agents to learn and improve their behavior within the game environment.']\u001b[0m\u001b[32;1m\u001b[1;3mThe paper \"LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning\" is authored by Hui Wang. It aims to develop a Gomoku AI system based on Large Language Models (LLMs), simulating the human learning process of playing chess. The system is designed to understand and apply Gomoku strategies through self-play and reinforcement learning.\n",
      "Action: Search\n",
      "Action Input: \"Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI\"\u001b[0m\u001b[36;1m\u001b[1;3m[' entity_type: related_questions.', \"One of the core components of our world models is 'intuitive physics' - an understanding of objects, space, and causality.\", 'We examine some of the most prominent AI paradigms dealing with object understanding, comparing their methods of evaluating objecthood to ...', \"The paper aims to bridge the gap between cognitive science's understanding of how humans and animals perceive objects (objecthood) and how AI ...\", 'The LLM-AAI framework combines DRL, simulation, and generative AI to enhance object understanding, incorporating dynamic interaction, context, ...', 'Description: Encourages dialogue between disparate communities in cognitive science and AI to create a more unified framework for studying ...', 'On Studocu you find all the lecture notes, summaries and study guides you need to pass your exams with better grades.', 'Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI ¬∑ ChatDOC. University of Cambridge; Technical University of Valencia ...', 'This paper emphasizes the development potential and importance of artificial intelligence to understand, possess and discriminate human mental states.', 'We will review the history of the relationship between cognitive science and artificial intelligence and show how early cognitive scientists understood how AI ...', 'Ë´ñÊñá„ÅÆÊ¶ÇË¶Å: „Åì„ÅÆË´ñÊñá„ÅØ„ÄÅAI„Å´„Åä„Åë„Çã„Ç™„Éñ„Ç∏„Çß„ÇØ„ÉàÁêÜËß£„Å´Èñ¢„Åô„ÇãÁ†îÁ©∂„Çí„ÄÅË™çÁü•ÁßëÂ≠¶„ÅÆÁü•Ë¶ã„Å´Âü∫„Å•„ÅÑ„Å¶Ë©ï‰æ°„Åô„Çã„Åì„Å®„ÇíÁõÆÁöÑ„Å®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇË´ñÊñá„ÅØÂ§ß„Åç„Åè2„Å§„ÅÆÈÉ®ÂàÜ ...']\u001b[0m\u001b[32;1m\u001b[1;3mThe paper \"Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI\" is authored by the University of Cambridge and the Technical University of Valencia. It aims to bridge the gap between cognitive science's understanding of how humans and animals perceive objects (objecthood) and how AI perceives them. The paper examines some of the most prominent AI paradigms dealing with object understanding and compares their methods of evaluating objecthood.\n",
      "Action: Search\n",
      "Action Input: \"Explicit group sparse projection with applications to deep learning and NMF\"\u001b[0m\u001b[36;1m\u001b[1;3m[' entity_type: related_questions.', 'We design a new sparse projection method for a set of vectors that guarantees a desired average sparsity level measured leveraging the popular Hoyer measure.', 'We design a new sparse projection method for a set of vectors that guarantees a desired average sparsity level measured leveraging the popular Hoyer measure.', 'This projection is inspired by the work of (Hoyer, 2004) in which each feature vector ci is independently projected, and used for sparse dictionary learning.', 'We design a new sparse projection method for a set of vectors that guarantees a desired average sparsity level measured leveraging the ...', 'A new sparse projection method for a set of vectors that guarantees a desired average sparsity level measured leveraging the popular Hoyer measure, ...', 'Implementation of Explicit Group Sparse Projection with Applications to Deep Learning and NMF published at Transactions of Machine Learning Research (TMLR) ...', 'Sparsity is increasingly relevant in machine learning, especially in non-negative matrix factorization. (NMF), where it aids in interpretability ...', '2022. TMLR. Explicit Group Sparse Projection with Applications to Deep Learning and NMF ... Grouped Sparse Projection for Deep Learning. Riyasat Ohib, Nicolas ...', 'GSP can be used in particular to sparsify the columns of a matrix, which we use to compute sparse low-rank matrix approximations (namely, sparse NMF). We ...', 'Explicit Group Sparse Projection with Applications to Deep Learning and NMF. R Ohib, N Gillis, N Dalmasso, S Shah, VK Potluru, S Plis. Transactions on Machine ...']\u001b[0m\u001b[32;1m\u001b[1;3mThe paper \"Explicit Group Sparse Projection with Applications to Deep Learning and NMF\" is authored by Riyasat Ohib, Nicolas Gillis, Niccolo Dalmasso, Sameena Shah, Vamsi Potluru, and Sergey Plis. It presents a new sparse projection method for a set of vectors that guarantees a desired average sparsity level measured leveraging the popular Hoyer measure. This projection is used for sparse dictionary learning and can be used to sparsify the columns of a matrix, which is used to compute sparse low-rank matrix approximations.\n",
      "Action: Search\n",
      "Action Input: \"Neural RF SLAM for unsupervised learning\"\u001b[0m\u001b[36;1m\u001b[1;3m[' entity_type: related_questions.', 'We present a neural network architecture for jointly learning user locations and environment mapping up to isometry, in an unsupervised way.', 'Abstract‚ÄîWe present a neural network architecture for jointly learning user locations and environment mapping up to isometry, in an unsupervised way, ...', 'We present a neural network architecture for jointly learning user locations and environment mapping up to isometry, in an unsupervised way, from channel ...', 'By associating multipath components (MPCs) in radio signals with the geometry of the radio reflectors, multipath propagation is exploited to increase ...', 'We present a neural network architecture for jointly learning user locations and environment mapping up to isometry, in an unsupervised way, from channel ...', '... Machine Learning Methods | Globecom 2022 ¬∑ Neural RF SLAM for unsupervised positioning and mapping with channel state information | ICC 2022 ¬∑ MIMO-GAN ...', 'This paper is concerned of the loop closure detection problem for visual simultaneous localization and mapping systems. We propose a novel approach based on ...', 'Neural RF SLAM for unsupervised positioning and mapping with channel state information S Kadambi, A Behboodi, JB Soriaga, M Welling, R Amiri, S Yerramalli', 'Our industrial precise positioning method, called Neural RF SLAM, achieves the best of the two technologies ‚Äî in our experiments with unlabeled ...', 'Neural RF SLAM for unsupervised positioning and mapping with channel state information ... The model is based on an encoder-decoder architecture. Decoder ¬∑ Super- ...']\u001b[0m\u001b[32;1m\u001b[1;3mThe paper \"Neural RF SLAM for unsupervised positioning and mapping with channel state information\" is authored by S Kadambi, A Behboodi, JB Soriaga, M Welling, R Amiri, and S Yerramalli. It presents a neural network architecture for jointly learning user locations and environment mapping up to isometry, in an unsupervised way, from channel state information. The model is based on an encoder-decoder architecture and exploits multipath propagation to increase the precision of the system.\n",
      "Action: Search\n",
      "Action Input: \"LLM-AAI framework combines DRL, simulation, and generative AI\"\u001b[0m\u001b[36;1m\u001b[1;3m[' entity_type: related_questions.', 'In this paper, we introduce LLMs in Animal-AI (LLM-AAI), a framework for conducting robust cognitive evaluations of the physical common-sense ...', \"Deep Reinforcement Learning (DRL) combines artificial neural networks and reinforcement learning, advancing AI's understanding of the visual ...\", 'This paper introduces a framework called LLM-AAI, which integrates LLMs with the Animal-AI (AAI) 3D simulation environment [87] . The AAI ...', 'LFMs are general-purpose AI models that can be used to model any kind of sequential data, including video, audio, text, time series, and signals.', 'Enter the power duo of Generative AI (GenAI) and Causal AI ‚Äì a combination reshaping how Fortune 1000 companies approach complex business problems.', \"Galileo's Evaluation Intelligence Platform empowers AI teams to evaluate, iterate, monitor, and protect generative AI applications at enterprise scale.\", 'Two ways to use the model in the network/simulation. ‚Ä¢ 1: Configure the parameters before the simulation: power, ACK timeout threshold, sensing power. ‚Ä¢ 2: Make ...', 'Artificial Intelligence (AI), Machine Learning (ML), Large Language Models (LLMs), and Generative AI are all related concepts in the field of computer science.', 'DFlow: A Generative Model Combining Denoising AutoEncoder and Normalizing Flow for High Fidelity Waveform Generation ... LLM and Simulation as Bilevel ...', 'In this article, I discuss the principal limitations to the use of generative artificial intelligence in medical education‚Äîhallucination, bias, ...']\u001b[0m\u001b[32;1m\u001b[1;3mThe paper \"LLMs in Animal-AI (LLM-AAI), a framework for conducting robust cognitive evaluations of the physical common-sense\" introduces a framework called LLM-AAI, which integrates Large Language Models (LLMs) with the Animal-AI (AAI) 3D simulation environment. This framework combines Deep Reinforcement Learning (DRL), simulation, and generative AI to advance AI's understanding of the visual world and conduct robust cognitive evaluations of the physical common-sense.\n",
      "Final Answer: Here are the 5 latest scientific publications on artificial intelligence:\n",
      "\n",
      "1. \"LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning\" by Hui Wang. This paper aims to develop a Gomoku AI system based on Large Language Models (LLMs), simulating the human learning process of playing chess. The system is designed to understand and apply Gomoku strategies through self-play and reinforcement learning.\n",
      "\n",
      "2. \"Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI\" by the University of Cambridge and the Technical University of Valencia. This paper aims to bridge the gap between cognitive science's understanding of how humans and animals perceive objects (objecthood) and how AI perceives them. The paper examines some of the most prominent AI paradigms dealing with object understanding and compares their methods of evaluating objecthood.\n",
      "\n",
      "3. \"Explicit Group Sparse Projection with Applications to Deep Learning and NMF\" by Riyasat Ohib, Nicolas Gillis, Niccolo Dalmasso, Sameena Shah, Vamsi Potluru, and Sergey Plis. This paper presents a new sparse projection method for a set of vectors that guarantees a desired average sparsity level measured leveraging the popular Hoyer measure. This projection is used for sparse dictionary learning and can be used to sparsify the columns of a matrix, which is used to compute sparse low-rank matrix approximations.\n",
      "\n",
      "4. \"Neural RF SLAM for unsupervised positioning and mapping with channel state information\" by S Kadambi, A Behboodi, JB Soriaga, M Welling, R Amiri, and S Yerramalli. This paper presents a neural network architecture for jointly learning user locations and environment mapping up to isometry, in an unsupervised way, from channel state information. The model is based on an encoder-decoder architecture and exploits multipath propagation to increase the precision of the system.\n",
      "\n",
      "5. \"LLMs in Animal-AI (LLM-AAI), a framework for conducting robust cognitive evaluations of the physical common-sense\". This paper introduces a framework called LLM-AAI, which integrates Large Language Models (LLMs) with the Animal-AI (AAI) 3D simulation environment. This framework combines Deep Reinforcement Learning (DRL), simulation, and generative AI to advance AI's understanding of the visual world and conduct robust cognitive evaluations of the physical common-sense.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '–ó–Ω–∞–π–¥–∏ 5 –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –Ω–∞—É–∫–æ–≤—ñ –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π –Ω–∞ —Ç–µ–º—É —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É: –Ω–∞–∑–≤—É, –∞–≤—Ç–æ—Ä—ñ–≤ —ñ –∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å.',\n",
       " 'output': 'Here are the 5 latest scientific publications on artificial intelligence:\\n\\n1. \"LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning\" by Hui Wang. This paper aims to develop a Gomoku AI system based on Large Language Models (LLMs), simulating the human learning process of playing chess. The system is designed to understand and apply Gomoku strategies through self-play and reinforcement learning.\\n\\n2. \"Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI\" by the University of Cambridge and the Technical University of Valencia. This paper aims to bridge the gap between cognitive science\\'s understanding of how humans and animals perceive objects (objecthood) and how AI perceives them. The paper examines some of the most prominent AI paradigms dealing with object understanding and compares their methods of evaluating objecthood.\\n\\n3. \"Explicit Group Sparse Projection with Applications to Deep Learning and NMF\" by Riyasat Ohib, Nicolas Gillis, Niccolo Dalmasso, Sameena Shah, Vamsi Potluru, and Sergey Plis. This paper presents a new sparse projection method for a set of vectors that guarantees a desired average sparsity level measured leveraging the popular Hoyer measure. This projection is used for sparse dictionary learning and can be used to sparsify the columns of a matrix, which is used to compute sparse low-rank matrix approximations.\\n\\n4. \"Neural RF SLAM for unsupervised positioning and mapping with channel state information\" by S Kadambi, A Behboodi, JB Soriaga, M Welling, R Amiri, and S Yerramalli. This paper presents a neural network architecture for jointly learning user locations and environment mapping up to isometry, in an unsupervised way, from channel state information. The model is based on an encoder-decoder architecture and exploits multipath propagation to increase the precision of the system.\\n\\n5. \"LLMs in Animal-AI (LLM-AAI), a framework for conducting robust cognitive evaluations of the physical common-sense\". This paper introduces a framework called LLM-AAI, which integrates Large Language Models (LLMs) with the Animal-AI (AAI) 3D simulation environment. This framework combines Deep Reinforcement Learning (DRL), simulation, and generative AI to advance AI\\'s understanding of the visual world and conduct robust cognitive evaluations of the physical common-sense.'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({'input': \"–ó–Ω–∞–π–¥–∏ 5 –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –Ω–∞—É–∫–æ–≤—ñ –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π –Ω–∞ —Ç–µ–º—É —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É: –Ω–∞–∑–≤—É, –∞–≤—Ç–æ—Ä—ñ–≤ —ñ –∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOqujC6qY_NY"
   },
   "source": [
    "\n",
    "\n",
    "### –ó–∞–≤–¥–∞–Ω–Ω—è 4: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞–≥–µ–Ω—Ç–∞-–ø–æ–º—ñ—á–Ω–∏–∫–∞ –¥–ª—è –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –±—ñ–∑–Ω–µ—Å-–∑–∞–¥–∞—á\n",
    "\n",
    "–°—Ç–≤–æ—Ä—ñ—Ç—å –∞–≥–µ–Ω—Ç–∞, —è–∫–∏–π –¥–æ–ø–æ–º–∞–≥–∞—î –≤–∏—Ä—ñ—à—É–≤–∞—Ç–∏ –∑–∞–¥–∞—á—ñ –±—ñ–∑–Ω–µ—Å-–∞–Ω–∞–ª—ñ—Ç–∏–∫–∏. –ê–≥–µ–Ω—Ç –º–∞—î –¥–æ–ø–æ–º–æ–≥—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—É —Å—Ç–≤–æ—Ä–∏—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Ä—ñ–∫ –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ —Ä—ñ–≤–µ–Ω—å —ñ–Ω—Ñ–ª—è—Ü—ñ—ó —ñ –ø–æ–≥–æ–¥–Ω—ñ —É–º–æ–≤–∏. –ê–≥–µ–Ω—Ç –º–∞—î –≤–º—ñ—Ç–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Python —ñ —Ö–æ–¥–∏—Ç–∏ –≤ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç –∞–±–∏ –æ—Ç—Ä–∏–º–∞—Ç–∏ –∞–∫—Ç—É–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ.\n",
    "\n",
    "**–ö—Ä–æ–∫–∏:**\n",
    "1. –ù–∞–ª–∞—à—Ç—É–π—Ç–µ –∞–≥–µ–Ω—Ç–∞, —è–∫–∏–π –ø—Ä–∞—Ü—é–≤–∞—Ç–∏–º–µ –∑ –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏, –∑–∞–¥–∞–Ω–∏–º–∏ —Ç–µ–∫—Å—Ç–æ–º. –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –ø–∏—à–µ\n",
    "\n",
    "```\n",
    "–ú–∏ –µ–∫—Å–ø–æ—Ä—Ç—É—î–º–æ –∞–ø–µ–ª—å—Å–∏–Ω–∏ –∑ –ë—Ä–∞–∑–∏–ª—ñ—ó. –í 2021 –µ–∫—Å–ø–æ—Ä—Ç—É–≤–∞–ª–∏ 200—Ç, –≤ 2022 - 190—Ç, –≤ 2023 - 210—Ç, –≤ 2024 —è–∫–∏–π —â–µ –Ω–µ –∑–∞–∫—ñ–Ω—á–∏–≤—Å—è - 220—Ç. –ó—Ä–æ–±–∏ –æ—Ü—ñ–Ω–∫—É —Å–∫—ñ–ª—å–∫–∏ –º–∏ –∑–º–æ–∂–µ–º–æ –µ–∫—Å–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –∞–ø–µ–ª—å—Å–∏–Ω—ñ–≤ –≤ 2025 –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ –ø–æ–≥–æ–¥–Ω—ñ —É–º–æ–≤–∏ –≤ –ë—Ä–∞–∑–∏–ª—ñ—ó —ñ –ø–æ–ø–∏—Ç –Ω–∞ –∞–ø–µ–ª—å—Å–∏–Ω–∏ –≤ —Å–≤—ñ—Ç—ñ –≤–∏—Ö–æ–¥—è—á–∏ –∑ –µ–∫–æ–Ω–æ–º—ñ—á–Ω–æ—ó —Å–∏—Ç—É–∞—Ü—ñ—ó.\n",
    "```\n",
    "\n",
    "2. –°—Ç–≤–æ—Ä—ñ—Ç—å –∑–∞–ø–∏—Ç –¥–æ –∞–≥–µ–Ω—Ç–∞, —â–æ –º—ñ—Å—Ç–∏—Ç—å —á—ñ—Ç–∫–µ –∑–∞–≤–¥–∞–Ω–Ω—è ‚Äì –≤–∏–¥–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±—ñ–∑–Ω–µ—Å –∞–Ω–∞–ª—ñ–∑—É –∞–±–æ –Ω–∞–ø–∏—Å–∞—Ç–∏, —â–æ –≤—ñ–Ω –Ω–µ –º–æ–∂–µ —Ü—å–æ–≥–æ –∑—Ä–æ–±–∏—Ç–∏ —ñ –∑–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ (–ø—Ä–æ—Å—Ç–æ –º–æ–∂–µ –±—É—Ç–∏ –≤—Å–µ –æ–¥–Ω–∏–º –ø–æ–≤—ñ–¥–æ–º–ª–ª–µ–Ω–Ω—è–º).\n",
    "\n",
    "3. –ó–∞–ø—É—Å—Ç—ñ—Ç—å –∞–≥–µ–Ω—Ç–∞ —ñ –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏. –©–æ –º–æ–∂–Ω–∞ –ø–æ–∫—Ä–∞—â–∏—Ç–∏?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UK6KA9PyRS0L"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
