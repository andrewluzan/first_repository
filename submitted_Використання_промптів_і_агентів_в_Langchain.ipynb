{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMalK_oYR-X_"
   },
   "source": [
    "\n",
    "### –ó–∞–≤–¥–∞–Ω–Ω—è 1: –í–∏–∫–ª–∏–∫ LLM –∑ –±–∞–∑–æ–≤–∏–º –ø—Ä–æ–º–ø—Ç–æ–º\n",
    "\n",
    "–°—Ç–≤–æ—Ä—ñ—Ç—å –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –≤–∏–∫–ª–∏–∫–∞—Ç–∏ LLM –∑—ñ –∑–≤–∏—á–∞–π–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–≤–∏–º –ø—Ä–æ–º–ø—Ç–æ–º.\n",
    "\n",
    "–ü—Ä–æ–º–ø—Ç –º–∞—î –¥–æ–∑–≤–ª—è—Ç–∏ –æ—Ç—Ä–∏–º–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ—Å—Ç–æ—é –º–æ–≤–æ—é –Ω–∞ –ø–µ–≤–Ω—É —Ç–µ–º—É. –í —Ü—å–æ–º—É –∑–∞–≤–¥–∞–Ω–Ω—ñ –º–∏ —Ö–æ—á–µ–º–æ –¥—ñ–∑–Ω–∞—Ç–∏—Å—å –ø—Ä–æ —Ç–µ–º—É \"–ö–≤–∞–Ω—Ç–æ–≤—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è\".\n",
    "\n",
    "–í—ñ–¥–ø–æ–≤—ñ–¥—å –º–æ–¥–µ–ª—ñ –ø–æ–≤–∏–Ω–Ω–∞ –º—ñ—Å—Ç–∏—Ç–∏ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è, –∫–ª—é—á–æ–≤—ñ –ø–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –ø–æ—Ç–æ—á–Ω—ñ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –≤ —Ü—ñ–π –≥–∞–ª—É–∑—ñ.\n",
    "\n",
    "–û–±–º–µ–∂—Ç–µ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –¥–æ 200 —Å–∏–º–≤–æ–ª—ñ–≤ —ñ –ø—Ä–æ–ø–∏—à—ñ—Ç—å –≤ –ø—Ä–æ–º–ø—Ç—ñ –∞–±–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –±—É–ª–∞ –∫–æ—Ä–æ—Ç–∫–æ—é (—Ü–µ –∑–µ–∫–æ–Ω–æ–º–∏—Ç—å –í–∞–º —á–∞—Å —ñ –≥—Ä–æ—à—ñ –Ω–∞ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ —Ç–æ–∫–µ–Ω–∏).\n",
    "\n",
    "–í —è–∫–æ—Å—Ç—ñ LLM –º–æ–∂–Ω–∞ —Å–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏—Å—å —è–∫ –º–æ–¥–µ–ª–ª—é –∑ HugginFace (—Ä–µ–∫–æ–º–µ–Ω–¥—É—é Mistral), —Ç–∞–∫ —ñ ChatGPT4 –∞–±–æ ChatGPT3. –í –æ–±–æ—Ö –≤–∏–ø–∞–¥–∫–∞—Ö —Ç—Ä–µ–±–∞ —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –ø–æ—Ç—Ä—ñ–±–Ω—É \"–æ–±–≥–æ—Ä—Ç–∫—É\" (—Ç–æ–±—Ç–æ –∫–ª–∞—Å, —è–∫–∏–π –¥–æ–∑–≤–æ–ª–∏—Ç—å —ñ–Ω—ñ—Ü—ñ—é–≤–∞—Ç–∏ –º–æ–¥–µ–ª—å) –∑ LangChain –¥–ª—è –≤–∏–∫–ª–∏–∫—É LLM –∑–∞ API, –∞ —Ç–∞–∫–æ–∂ –∑—á–∏—Ç–∞—Ç–∏ –æ—Å–æ–±–∏—Å—Ç–∏–π —Ç–æ–∫–µ–Ω –∑ —Ñ–∞–π–ª–∞, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, `creds.json`, —è–∫–∏–π —Ä–æ–∑–º—ñ—â–µ–Ω–∏–π —É –í–∞—Å –ª–æ–∫–∞–ª—å–Ω–æ —ñ –í–∏ –ù–ï –∑–¥–∞—î—Ç–µ –π–æ–≥–æ –≤ –î–ó —ñ –ù–ï –∫–æ–º—ñ—Ç–∏—Ç–µ –≤ git üòè\n",
    "\n",
    "–í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å —Å–≤–æ—î –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∏ –Ω–∞ —Å–≤—ñ–π —Ä–æ–∑—Å—É–¥ (—Ç—É—Ç –Ω–µ–º–∞—î –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —á–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è) —ñ –Ω–∞–ø–∏—à—ñ—Ç—å, —á–æ–º—É –≤–∏ –æ–±—Ä–∞–ª–∏ —Å–∞–º–µ —Ç–∞–∫–µ –∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è —Ü—å–æ–≥–æ –∑–∞–≤–¥–∞–Ω–Ω—è.  \n",
    "\n",
    "–ó–∞–ø–∏—Ç–∏ –º–æ–∂–Ω–∞ —Ä–æ–±–∏—Ç–∏ —è–∫ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é, —Ç–∞–∫ —ñ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é - –æ—Ä—ñ—î–Ω—Ç—É–π—Ç–µ—Å—å –Ω–∞ —Ç–µ, –¥–µ —ñ —á–∏ —Ö–æ—á–µ—Ç–µ –≤–∏ –ø–æ—Ç—ñ–º –ª–∏—à–∏—Ç–∏ —Ü–µ–π –ø—Ä–æ—î–∫—Ç —ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∞ —è–∫–∞ –º–æ–≤–∞ –±—É–¥–µ –ø–∞—Å—É–≤–∞—Ç–∏ –±—ñ–ª—å—à–µ. –í —Ä–æ–∑–≤ º—è–∑–∫–∞—Ö –ø—Ä–æ–º–ø—Ç–∏ - —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vT1ICe74cAUS"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open('creds.json') as file:\n",
    "  creds = json.load(file)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = creds[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ 0.3 ‚Äì –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–∏—Ö —Ç–∞ –ª–∞–∫–æ–Ω—ñ—á–Ω–∏—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π. –ü—ñ–¥—ñ–π–¥–µ –¥–ª—è —Ü—å–æ–≥–æ –∑–∞–≤–¥–∞–Ω–Ω—è, –æ—Å–∫—ñ–ª—å–∫–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ –æ—Ç—Ä–∏–º–∞—Ç–∏ —á—ñ—Ç–∫—É, —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é.\n",
    "overal_temperature = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=overal_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "–ö–≤–∞–Ω—Ç–æ–≤—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è - —Ü–µ –º–µ—Ç–æ–¥ –æ–±—Ä–æ–±–∫–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó, —è–∫–∏–π –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –∫–≤–∞–Ω—Ç–æ–≤—ñ —Å–∏—Å—Ç–µ–º–∏ –∑–∞–º—ñ—Å—Ç—å –∫–ª–∞—Å–∏—á–Ω–∏—Ö –±—ñ—Ç—ñ–≤. –í–æ–Ω–∏ –¥–æ–∑–≤–æ–ª—è—é—Ç—å –≤–∏–∫–æ–Ω—É–≤–∞—Ç–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è —à–≤–∏–¥—à–µ —Ç–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—à–µ, –∑–æ–∫—Ä–µ–º–∞ –¥–ª—è –∑–∞–¥–∞—á, —è–∫—ñ –Ω–µ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –≤–∏—Ä—ñ—à–µ–Ω—ñ –∫–ª–∞—Å–∏—á–Ω–∏–º–∏ –∫–æ–º–ø'—é—Ç–µ—Ä–∞–º–∏. –ü–æ—Ç–æ—á–Ω—ñ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è —Å–ø—Ä—è–º–æ–≤–∞–Ω—ñ –Ω–∞ —Ä–æ–∑—Ä–æ–±–∫—É –∫–≤–∞–Ω—Ç–æ–≤–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤ —Ç–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –±—ñ–ª—å—à –ø–æ—Ç—É–∂–Ω–∏—Ö —Ç–∞ —Å—Ç–∞–±—ñ–ª—å–Ω–∏—Ö –∫–≤–∞–Ω—Ç–æ–≤–∏—Ö –∫–æ–º–ø'—é—Ç–µ—Ä—ñ–≤.\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–æ–º–ø—Ç –¥–ª—è –≤–∏–∫–ª–∏–∫—É –º–æ–¥–µ–ª—ñ\n",
    "prompt = \"–ü–æ—è—Å–Ω–∏, —â–æ —Ç–∞–∫–µ –∫–≤–∞–Ω—Ç–æ–≤—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è, –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è, —ó—Ö –ø–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –ø–æ—Ç–æ—á–Ω—ñ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è. –í—ñ–¥–ø–æ–≤—ñ–¥—å –º–∞—î –±—É—Ç–∏ –¥—É–∂–µ –∫–æ—Ä–æ—Ç–∫–æ—é, –Ω–µ –±—ñ–ª—å—à–µ 200 —Å–∏–º–≤–æ–ª—ñ–≤.\"\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ\n",
    "response = llm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiIzV0UIS0GA"
   },
   "source": [
    "### –ó–∞–≤–¥–∞–Ω–Ω—è 2: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É\n",
    "–¢–µ–ø–µ—Ä –º–∏ —Ö–æ—á–µ–º–æ –æ–Ω–æ–≤–∏—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Ñ—É–∫–Ω—Ü—ñ–æ–Ω–∞–ª —Ç–∞–∫, –∞–±–∏ –≤ –ø—Ä–æ–º–ø—Ç –º–∏ –º–æ–≥–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç–∏ —Ç–µ–º—É —è–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä. –î–ª—è —Ü—å–æ–≥–æ —Å–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ—Å—å `PromptTemplate` –∑ `langchain` —ñ —Ä–µ–∞–ª—ñ–∑—É–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–∏–π –ø—Ä–æ–º–ø—Ç —Ç–∞ –≤–∏–∫–ª–∏–∫ –º–æ–¥–µ–ª—ñ –∑ –Ω–∏–º.\n",
    "\n",
    "–ó–∞–ø—É—Å—Ç—ñ—Ç—å –æ–Ω–æ–≤–ª–µ–Ω–∏–π —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª (–ø—Ä–æ–º–ø—Ç + –º–æ–¥–µ–ª) –¥–ª—è –ø–æ—è—Å–Ω–µ–Ω—å –ø—Ä–æ —Ç–µ–º–∏\n",
    "- \"–ë–∞—î—Å—ñ–≤—Å—å–∫—ñ –º–µ—Ç–æ–¥–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\"\n",
    "- \"–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\"\n",
    "- \"Explainable AI\"\n",
    "\n",
    "–í–∏–≤–µ–¥—ñ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤—ñ–¥–ø—Ä–∞—Ü—é–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞ –µ–∫—Ä–∞–Ω."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kcg7EfdNcBDq"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"Theme\"],\n",
    "    template=\"–ü–æ—è—Å–Ω–∏, —â–æ —Ç–∞–∫–µ {Theme}? –í—ñ–¥–ø–æ–≤—ñ–¥—å –º–∞—î –±—É—Ç–∏ –¥—É–∂–µ –∫–æ—Ä–æ—Ç–∫–æ—é, –Ω–µ –±—ñ–ª—å—à–µ 200 —Å–∏–º–≤–æ–ª—ñ–≤.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–æ—è—Å–Ω–∏, —â–æ —Ç–∞–∫–µ –ë–∞—î—Å—ñ–≤—Å—å–∫—ñ –º–µ—Ç–æ–¥–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ? –í—ñ–¥–ø–æ–≤—ñ–¥—å –º–∞—î –±—É—Ç–∏ –¥—É–∂–µ –∫–æ—Ä–æ—Ç–∫–æ—é, –Ω–µ –±—ñ–ª—å—à–µ 200 —Å–∏–º–≤–æ–ª—ñ–≤.\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(Theme=\"–ë–∞—î—Å—ñ–≤—Å—å–∫—ñ –º–µ—Ç–æ–¥–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "–ë–∞—î—Å—ñ–≤—Å—å–∫—ñ –º–µ—Ç–æ–¥–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ - —Ü–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω—ñ –º–µ—Ç–æ–¥–∏, —è–∫—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å —Ç–µ–æ—Ä–µ–º—É –ë–∞—î—Å–∞ –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –≤–∏–Ω–∏–∫–Ω–µ–Ω–Ω—è –ø–æ–¥—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞—è–≤–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó —Ç–∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö –∑–Ω–∞–Ω—å. –í–æ–Ω–∏ –¥–æ–∑–≤–æ–ª—è—é—Ç—å –≤—Ä–∞—Ö–æ–≤—É–≤–∞—Ç–∏ –Ω–µ –≤–∏–∫–ª—é—á–Ω–æ –¥–∞–Ω—ñ, –∞–ª–µ —ñ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞–Ω–Ω—è, —â–æ –∑—Ä–æ–±–∏–ª–∏ —ó—Ö –ø–æ–ø—É–ª—è—Ä–Ω–∏–º–∏ –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –æ–±–º–µ–∂–µ–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(prompt.format(Theme=\"–ë–∞—î—Å—ñ–≤—Å—å–∫—ñ –º–µ—Ç–æ–¥–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–æ—è—Å–Ω–∏, —â–æ —Ç–∞–∫–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ? –í—ñ–¥–ø–æ–≤—ñ–¥—å –º–∞—î –±—É—Ç–∏ –¥—É–∂–µ –∫–æ—Ä–æ—Ç–∫–æ—é, –Ω–µ –±—ñ–ª—å—à–µ 200 —Å–∏–º–≤–æ–ª—ñ–≤.\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(Theme=\"–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ - —Ü–µ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ–Ω–Ω–æ—ó –º–µ—Ä–µ–∂—ñ, —è–∫–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –º–µ—Ö–∞–Ω—ñ–∑–º —Å–∞–º–æ—É–≤–∞–≥–∏ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π –¥–∞–Ω–∏—Ö, —Ç–∞–∫–∏—Ö —è–∫ —Ç–µ–∫—Å—Ç –∞–±–æ –º–æ–≤–∞. –í–æ–Ω–∏ –∑–¥–∞—Ç–Ω—ñ –¥–æ—Å—è–≥–∞—Ç–∏ –≤–∏—Å–æ–∫–æ—ó —Ç–æ—á–Ω–æ—Å—Ç—ñ –≤ –∑–∞–≤–¥–∞–Ω–Ω—è—Ö –æ–±—Ä–æ–±–∫–∏ –º–æ–≤–∏ —Ç–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–∫–ª–∞–¥—É.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(prompt.format(Theme=\"–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–æ—è—Å–Ω–∏, —â–æ —Ç–∞–∫–µ Explainable AI? –í—ñ–¥–ø–æ–≤—ñ–¥—å –º–∞—î –±—É—Ç–∏ –¥—É–∂–µ –∫–æ—Ä–æ—Ç–∫–æ—é, –Ω–µ –±—ñ–ª—å—à–µ 200 —Å–∏–º–≤–æ–ª—ñ–≤.\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(Theme=\"Explainable AI\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Explainable AI (XAI) - —Ü–µ –ø—ñ–¥—Ö—ñ–¥ –¥–æ —Ä–æ–∑—Ä–æ–±–∫–∏ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–∏—Ö —Å–∏—Å—Ç–µ–º, —è–∫–∏–π –¥–æ–∑–≤–æ–ª—è—î –∑—Ä–æ–∑—É–º—ñ—Ç–∏ –ø—Ä–∏—á–∏–Ω–∏ –ø—Ä–∏–π–Ω—è—Ç–∏—Ö —Ä—ñ—à–µ–Ω—å —Ç–∞ –ø–æ—è—Å–Ω–∏—Ç–∏ —ó—Ö –ª—é–¥—è–º. –¶–µ –≤–∞–∂–ª–∏–≤–æ –¥–ª—è –∑–∞–±–µ–∑–ø–µ—á–µ–Ω–Ω—è –¥–æ–≤—ñ—Ä–∏ —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω–æ—Å—Ç—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(prompt.format(Theme=\"Explainable AI\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9UsL2gXSe-3"
   },
   "source": [
    "\n",
    "\n",
    "### –ó–∞–≤–¥–∞–Ω–Ω—è 3: –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó –ø—Ä–æ—Ü–µ—Å—ñ–≤\n",
    "–°—Ç–≤–æ—Ä—ñ—Ç—å –∞–≥–µ–Ω—Ç–∞, —è–∫–∏–π –¥–æ–ø–æ–º–æ–∂–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —à—É–∫–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –æ—Å—Ç–∞–Ω–Ω—ñ –Ω–∞—É–∫–æ–≤—ñ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó –≤ —Ä—ñ–∑–Ω–∏—Ö –≥–∞–ª—É–∑—è—Ö. –ù–∞–ø—Ä–∏–∫–ª–∞–¥, –∞–≥–µ–Ω—Ç –º–∞—î –∑–Ω–∞–π—Ç–∏ 5 –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π –Ω–∞ —Ç–µ–º—É —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É.\n",
    "\n",
    "**–ö—Ä–æ–∫–∏:**\n",
    "1. –ù–∞–ª–∞—à—Ç—É–π—Ç–µ –∞–≥–µ–Ω—Ç–∞ —Ç–∏–ø—É ReAct –≤ LangChain –¥–ª—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤.\n",
    "2. –°—Ç–≤–æ—Ä—ñ—Ç—å –ø—Ä–æ–º–ø—Ç, —è–∫–∏–π —Å–ø—Ä—è–º–æ–≤—É—î –∞–≥–µ–Ω—Ç–∞ —à—É–∫–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –≤ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—ñ –∞–±–æ –≤ –±–∞–∑–∞—Ö –¥–∞–Ω–∏—Ö –Ω–∞—É–∫–æ–≤–∏—Ö –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π.\n",
    "3. –ê–≥–µ–Ω—Ç –ø–æ–≤–∏–Ω–µ–Ω –≤–∏–¥–∞—Ç–∏ —Å–ø–∏—Å–æ–∫ –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π, –∫–æ–∂–Ω–∞ –∑ —è–∫–∏—Ö –º—ñ—Å—Ç–∏—Ç—å –Ω–∞–∑–≤—É, –∞–≤—Ç–æ—Ä—ñ–≤ —ñ –∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å.\n",
    "\n",
    "–î–ª—è –≤–∑–∞—î–º–æ–¥—ñ—ó –∑ –ø–æ—à—É–∫–æ–º —Ç–∞–º –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ `Tool`. –í –ª–µ–∫—Ü—ñ—ó –º–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–ª–∏ `serpapi`. –ú–æ–∂–Ω–∞ –ø—Ä–æ–¥–æ–≤–∂–∏—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—Ç–∏—Å—å –Ω–∏–º, –∞–±–æ –æ–±—Ä–∞—Ç–∏ —ñ–Ω—à–µ –ê–†–Ü –¥–ª—è –ø–æ—à—É–∫—É (–≤–æ–Ω–∏ –≤ —Ç–æ–º—É —á–∏—Å–ª—ñ —î –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω—ñ). –ü–µ—Ä–µ–ª—ñ–∫ —Ä—ñ–∑–Ω–∏—Ö –ê–†–Ü, –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –≤ langchain, —ñ –æ—Ä—ñ—î–Ω—Ç–∏—Ä –ø–æ –≤–∞—Ä—Ç–æ—Å—Ç—ñ –∑–∞–ø–∏—Ç—ñ–≤ –º–æ–∂–Ω–∞ –∑–Ω–∞–π—Ç–∏ –≤ –æ–∫—Ä–µ–º–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—ñ [—Ç—É—Ç](https://hannapylieva.notion.site/API-12994835849480a69b2adf2b8441cbb3?pvs=4).\n",
    "\n",
    "–õ–∏—à–∞—é —Ç–∞–∫–æ–∂ –Ω–∏–∂—á–µ –ø—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –æ–¥–Ω–æ–≥–æ –∑ –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∏—Ö –ø–æ—à—É–∫–æ–≤–∏—Ö –ê–†–Ü - DuckDuckGo (–Ω–µ –ø–æ—Ç—Ä–µ–±—É—î —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ–∫–µ–Ω–∞!)  - –º–æ–∂–ª–∏–≤–æ –≤—ñ–Ω –≤–∞–º —Å–ø–æ–¥–æ–±–∞—î—Ç—å—Å—è :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "60jZEitmcCiP",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in /Users/luzan/Library/Python/3.11/lib/python/site-packages (0.3.14)\n",
      "Requirement already satisfied: openai in /Users/luzan/Library/Python/3.11/lib/python/site-packages (1.59.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langchain) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langchain) (3.11.11)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langchain) (0.3.29)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langchain) (0.3.5)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langchain) (0.2.10)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langchain) (2.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (0.28.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (24.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.14)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/luzan/Library/Python/3.11/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.agents.react.base import ReActChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import Tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SERPAPI_API_KEY\"] = creds[\"SERPAPI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luzan/Library/Python/3.11/lib/python/site-packages/langsmith/client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tools = load_tools([\"serpapi\"], llm=llm)\n",
    "prompt = hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to search for the latest scientific publications on artificial intelligence, including their titles, authors, and a brief description. I will use the search tool to find this information.\n",
      "Action: Search\n",
      "Action Input: \"latest scientific publications on artificial intelligence\"\u001b[0m\u001b[36;1m\u001b[1;3m[' entity_type: related_questions.', \"The journal's scope encompasses all areas of AI, including agents and multi-agent systems, automated reasoning, constraint processing and search ...\", 'Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI. Danaja Rutar, Alva Markelius, Konstantinos Voudouris, Jos√© ...', \"Access leading publications on AI and big data that are shaping industry understanding and capabilities, curated by Certainly's experts.\", 'The AI Index is recognized globally as one of the most credible and authoritative sources for data and insights on artificial intelligence. Previous editions ...', 'The journal of Artificial Intelligence (AIJ) welcomes papers on broad aspects of AI that constitute advances in the overall field.', 'AI system predicts protein fragments that can bind to or inhibit a target. FragFold, developed by MIT Biology researchers, is a computational method with ...', 'AI models are growing ever-more capable, accurate and impressive. The question of if they represent ‚Äúgeneral intelligence‚Äù is increasingly moot. By Lauren ...', 'Artificial intelligence software is increasingly involved in reviewing papers, provoking interest and unease. By. Miryam Naddaf.', \"AI Needs to Be More Energy-Efficient ¬∑ Is ChatGPT a Drug? ¬∑ $0 for Digital Access ¬∑ The AI Future Is Here ¬∑ Google's AI Can Beat the Smartest High Schoolers in Math.\"]\u001b[0m\u001b[32;1m\u001b[1;3mThe search results do not provide specific information about the latest scientific publications on artificial intelligence. I need to refine my search to get more specific results.\n",
      "Action: Search\n",
      "Action Input: \"latest scientific articles on artificial intelligence 2022\"\u001b[0m\u001b[36;1m\u001b[1;3m[' entity_type: related_questions.', 'Artificial Intelligence News. Everything on AI including futuristic robots with artificial intelligence, computer models of human intelligence and more.', \"The journal's scope encompasses all areas of AI, including agents and multi-agent systems, automated reasoning, constraint processing and search ...\", 'Are You Smarter Than A.I.? Some experts predict that A.I. will surpass human intelligence within the next few years. ¬∑ How Artificial Intelligence Reasons.', 'The AI Index is recognized globally as one of the most credible and authoritative sources for data and insights on artificial intelligence. Previous editions ...', 'The journal of Artificial Intelligence (AIJ) welcomes papers on broad aspects of AI that constitute advances in the overall field.', 'Artificial Intelligence Review is a fully open access journal publishing state-of-the-art research in artificial intelligence and cognitive science.', 'In the latest survey, 78 percent of respondents say their organizations use AI in at least one business function, up from 72 percent in ...', 'This study examines the impact of artificial intelligence (AI) on loss in decision-making, laziness, and privacy concerns among university ...', 'Explores the disruptive technological revolution of AI. A nexus for research in core and applied AI areas, this journal focuses on the enormous expansion of AI ...', 'A new National Academies report recommends ways for the U.S. to reap the benefits of artificial intelligence in biotechnology while minimizing risks that AI may ...']\u001b[0m\u001b[32;1m\u001b[1;3mThe search results still do not provide specific information about the latest scientific publications on artificial intelligence. I need to refine my search to get more specific results.\n",
      "Action: Search\n",
      "Action Input: \"recent scientific papers on artificial intelligence 2022\"\u001b[0m\u001b[36;1m\u001b[1;3m[' entity_type: related_questions.', \"The journal's scope encompasses all areas of AI, including agents and multi-agent systems, automated reasoning, constraint processing and search ...\", 'A curated list of the latest breakthroughs in AI (in 2022) by release date with a clear video explanation, link to a more in-depth article, and code.', 'The journal of Artificial Intelligence (AIJ) welcomes papers on broad aspects of AI that constitute advances in the overall field.', 'Qualcomm contributes impactful artificial intelligence research covering computer vision, machine learning, generative modeling, and more.', 'Artificial Intelligence News. Everything on AI including futuristic robots with artificial intelligence, computer models of human intelligence and more.', 'Today I return to my series of AI paper recommendations bringing you ten new AI paper suggestions (and many more links).', 'Artificial Intelligence Review is a fully open access journal publishing state-of-the-art research in artificial intelligence and cognitive science.', 'Papers With Code highlights trending Machine Learning research and the code to implement it.', 'We caught up with experts in the RE‚Ä¢WORK community to find out what the top 17 AI papers are for 2022 so far that you can add to your Summer must reads.', 'We write about the progress and impact of artificial intelligence through a cross-sector lens, from research and development to technical performance, the ...']\u001b[0m\u001b[32;1m\u001b[1;3mThe search results still do not provide specific information about the latest scientific publications on artificial intelligence. I need to refine my search to get more specific results.\n",
      "Action: Search\n",
      "Action Input: \"recently published artificial intelligence research papers 2022\"\u001b[0m\u001b[36;1m\u001b[1;3m[' entity_type: related_questions.', 'A curated list of the latest breakthroughs in AI (in 2022) by release date with a clear video explanation, link to a more in-depth article, and code.', \"The journal's scope encompasses all areas of AI, including agents and multi-agent systems, automated reasoning, constraint processing and search ...\", 'Below are papers that Qualcomm AI Research has written or co-authored. ... Machine Learning Methods | Globecom 2022 ¬∑ Neural RF SLAM for unsupervised ...', 'LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning ¬∑ Cognitive Science-Inspired Evaluation of Core ...', 'Stay informed on the latest trending ML papers with code, research developments, libraries, methods, and datasets. Read previous issues. Subscribe ...', 'Our work provides valuable insights and future research directions for improving the reliability of AI supercomputer clusters, emphasizing the need for flexible ...', '2022 ; Explicit group sparse projection with applications to deep learning and NMF ¬∑ Riyasat Ohib, Nicolas Gillis, Niccolo Dalmasso, Sameena Shah, Vamsi Potluru, ...', 'This repository is an up-to-date list of significant AI papers organized by publication date. It covers five fields : computer vision, natural language ...', 'The journal of Artificial Intelligence (AIJ) welcomes papers on broad aspects of AI that constitute advances in the overall field.', 'We caught up with experts in the RE‚Ä¢WORK community to find out what the top 17 AI papers are for 2022 so far that you can add to your Summer must reads.']\u001b[0m\u001b[32;1m\u001b[1;3mThe search results now provide some specific information about the latest scientific publications on artificial intelligence. I can see some titles and authors, but I still need more detailed information to provide a brief description for each publication.\n",
      "Action: Search\n",
      "Action Input: \"LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning\"\u001b[0m\u001b[36;1m\u001b[1;3m['This study aims to develop a Gomoku AI system based on LLMs, simulating the human learning process of playing chess. The system is de-signed to ...', 'This study targets developing a Gomoku AI system using LLMs, focusing on human-like learning. The system aims to understand and apply Gomoku ...', 'LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning PAPER: https://arxiv.org/abs ...', '1. LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning ¬∑ 2. Cognitive Science-Inspired Evaluation of ...', 'Title: LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning. Hui Wang. Subjects: Artificial ...', 'LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning. Yesterday by Hui Wang ¬∑ Artificial Intelligence ...', 'LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning. RL. NLP. ‚ö° Summary (one liner).', 'This thesis aims to build an artificial intelligence for Gomoku using neural networks and reinforcement learning, resulting in a solution that ...', 'Reinforcement learning (RL) allows non-player characters (NPCs) or game agents to learn and improve their behavior within the game environment.']\u001b[0m\u001b[32;1m\u001b[1;3mThe paper \"LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning\" is authored by Hui Wang. It aims to develop a Gomoku AI system based on Large Language Models (LLMs), simulating the human learning process of playing chess. The system is designed to understand and apply Gomoku strategies through self-play and reinforcement learning.\n",
      "Action: Search\n",
      "Action Input: \"Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI\"\u001b[0m\u001b[36;1m\u001b[1;3m[' entity_type: related_questions.', \"One of the core components of our world models is 'intuitive physics' - an understanding of objects, space, and causality.\", 'We examine some of the most prominent AI paradigms dealing with object understanding, comparing their methods of evaluating objecthood to ...', \"The paper aims to bridge the gap between cognitive science's understanding of how humans and animals perceive objects (objecthood) and how AI ...\", 'The LLM-AAI framework combines DRL, simulation, and generative AI to enhance object understanding, incorporating dynamic interaction, context, ...', 'Description: Encourages dialogue between disparate communities in cognitive science and AI to create a more unified framework for studying ...', 'On Studocu you find all the lecture notes, summaries and study guides you need to pass your exams with better grades.', 'Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI ¬∑ ChatDOC. University of Cambridge; Technical University of Valencia ...', 'This paper emphasizes the development potential and importance of artificial intelligence to understand, possess and discriminate human mental states.', 'We will review the history of the relationship between cognitive science and artificial intelligence and show how early cognitive scientists understood how AI ...', 'Ë´ñÊñá„ÅÆÊ¶ÇË¶Å: „Åì„ÅÆË´ñÊñá„ÅØ„ÄÅAI„Å´„Åä„Åë„Çã„Ç™„Éñ„Ç∏„Çß„ÇØ„ÉàÁêÜËß£„Å´Èñ¢„Åô„ÇãÁ†îÁ©∂„Çí„ÄÅË™çÁü•ÁßëÂ≠¶„ÅÆÁü•Ë¶ã„Å´Âü∫„Å•„ÅÑ„Å¶Ë©ï‰æ°„Åô„Çã„Åì„Å®„ÇíÁõÆÁöÑ„Å®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇË´ñÊñá„ÅØÂ§ß„Åç„Åè2„Å§„ÅÆÈÉ®ÂàÜ ...']\u001b[0m\u001b[32;1m\u001b[1;3mThe paper \"Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI\" is authored by the University of Cambridge and the Technical University of Valencia. It aims to bridge the gap between cognitive science's understanding of how humans and animals perceive objects (objecthood) and how AI perceives them. The paper examines some of the most prominent AI paradigms dealing with object understanding and compares their methods of evaluating objecthood.\n",
      "Action: Search\n",
      "Action Input: \"Explicit group sparse projection with applications to deep learning and NMF\"\u001b[0m\u001b[36;1m\u001b[1;3m[' entity_type: related_questions.', 'We design a new sparse projection method for a set of vectors that guarantees a desired average sparsity level measured leveraging the popular Hoyer measure.', 'We design a new sparse projection method for a set of vectors that guarantees a desired average sparsity level measured leveraging the popular Hoyer measure.', 'This projection is inspired by the work of (Hoyer, 2004) in which each feature vector ci is independently projected, and used for sparse dictionary learning.', 'We design a new sparse projection method for a set of vectors that guarantees a desired average sparsity level measured leveraging the ...', 'A new sparse projection method for a set of vectors that guarantees a desired average sparsity level measured leveraging the popular Hoyer measure, ...', 'Implementation of Explicit Group Sparse Projection with Applications to Deep Learning and NMF published at Transactions of Machine Learning Research (TMLR) ...', 'Sparsity is increasingly relevant in machine learning, especially in non-negative matrix factorization. (NMF), where it aids in interpretability ...', '2022. TMLR. Explicit Group Sparse Projection with Applications to Deep Learning and NMF ... Grouped Sparse Projection for Deep Learning. Riyasat Ohib, Nicolas ...', 'GSP can be used in particular to sparsify the columns of a matrix, which we use to compute sparse low-rank matrix approximations (namely, sparse NMF). We ...', 'Explicit Group Sparse Projection with Applications to Deep Learning and NMF. R Ohib, N Gillis, N Dalmasso, S Shah, VK Potluru, S Plis. Transactions on Machine ...']\u001b[0m\u001b[32;1m\u001b[1;3mThe paper \"Explicit Group Sparse Projection with Applications to Deep Learning and NMF\" is authored by Riyasat Ohib, Nicolas Gillis, Niccolo Dalmasso, Sameena Shah, Vamsi Potluru, and Sergey Plis. It presents a new sparse projection method for a set of vectors that guarantees a desired average sparsity level measured leveraging the popular Hoyer measure. This projection is used for sparse dictionary learning and can be used to sparsify the columns of a matrix, which is used to compute sparse low-rank matrix approximations.\n",
      "Action: Search\n",
      "Action Input: \"Neural RF SLAM for unsupervised learning\"\u001b[0m\u001b[36;1m\u001b[1;3m[' entity_type: related_questions.', 'We present a neural network architecture for jointly learning user locations and environment mapping up to isometry, in an unsupervised way.', 'Abstract‚ÄîWe present a neural network architecture for jointly learning user locations and environment mapping up to isometry, in an unsupervised way, ...', 'We present a neural network architecture for jointly learning user locations and environment mapping up to isometry, in an unsupervised way, from channel ...', 'By associating multipath components (MPCs) in radio signals with the geometry of the radio reflectors, multipath propagation is exploited to increase ...', 'We present a neural network architecture for jointly learning user locations and environment mapping up to isometry, in an unsupervised way, from channel ...', '... Machine Learning Methods | Globecom 2022 ¬∑ Neural RF SLAM for unsupervised positioning and mapping with channel state information | ICC 2022 ¬∑ MIMO-GAN ...', 'This paper is concerned of the loop closure detection problem for visual simultaneous localization and mapping systems. We propose a novel approach based on ...', 'Neural RF SLAM for unsupervised positioning and mapping with channel state information S Kadambi, A Behboodi, JB Soriaga, M Welling, R Amiri, S Yerramalli', 'Our industrial precise positioning method, called Neural RF SLAM, achieves the best of the two technologies ‚Äî in our experiments with unlabeled ...', 'Neural RF SLAM for unsupervised positioning and mapping with channel state information ... The model is based on an encoder-decoder architecture. Decoder ¬∑ Super- ...']\u001b[0m\u001b[32;1m\u001b[1;3mThe paper \"Neural RF SLAM for unsupervised positioning and mapping with channel state information\" is authored by S Kadambi, A Behboodi, JB Soriaga, M Welling, R Amiri, and S Yerramalli. It presents a neural network architecture for jointly learning user locations and environment mapping up to isometry, in an unsupervised way, from channel state information. The model is based on an encoder-decoder architecture and exploits multipath propagation to increase the precision of the system.\n",
      "Action: Search\n",
      "Action Input: \"LLM-AAI framework combines DRL, simulation, and generative AI\"\u001b[0m\u001b[36;1m\u001b[1;3m[' entity_type: related_questions.', 'In this paper, we introduce LLMs in Animal-AI (LLM-AAI), a framework for conducting robust cognitive evaluations of the physical common-sense ...', \"Deep Reinforcement Learning (DRL) combines artificial neural networks and reinforcement learning, advancing AI's understanding of the visual ...\", 'This paper introduces a framework called LLM-AAI, which integrates LLMs with the Animal-AI (AAI) 3D simulation environment [87] . The AAI ...', 'LFMs are general-purpose AI models that can be used to model any kind of sequential data, including video, audio, text, time series, and signals.', 'Enter the power duo of Generative AI (GenAI) and Causal AI ‚Äì a combination reshaping how Fortune 1000 companies approach complex business problems.', \"Galileo's Evaluation Intelligence Platform empowers AI teams to evaluate, iterate, monitor, and protect generative AI applications at enterprise scale.\", 'Two ways to use the model in the network/simulation. ‚Ä¢ 1: Configure the parameters before the simulation: power, ACK timeout threshold, sensing power. ‚Ä¢ 2: Make ...', 'Artificial Intelligence (AI), Machine Learning (ML), Large Language Models (LLMs), and Generative AI are all related concepts in the field of computer science.', 'DFlow: A Generative Model Combining Denoising AutoEncoder and Normalizing Flow for High Fidelity Waveform Generation ... LLM and Simulation as Bilevel ...', 'In this article, I discuss the principal limitations to the use of generative artificial intelligence in medical education‚Äîhallucination, bias, ...']\u001b[0m\u001b[32;1m\u001b[1;3mThe paper \"LLMs in Animal-AI (LLM-AAI), a framework for conducting robust cognitive evaluations of the physical common-sense\" introduces a framework called LLM-AAI, which integrates Large Language Models (LLMs) with the Animal-AI (AAI) 3D simulation environment. This framework combines Deep Reinforcement Learning (DRL), simulation, and generative AI to advance AI's understanding of the visual world and conduct robust cognitive evaluations of the physical common-sense.\n",
      "Final Answer: Here are the 5 latest scientific publications on artificial intelligence:\n",
      "\n",
      "1. \"LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning\" by Hui Wang. This paper aims to develop a Gomoku AI system based on Large Language Models (LLMs), simulating the human learning process of playing chess. The system is designed to understand and apply Gomoku strategies through self-play and reinforcement learning.\n",
      "\n",
      "2. \"Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI\" by the University of Cambridge and the Technical University of Valencia. This paper aims to bridge the gap between cognitive science's understanding of how humans and animals perceive objects (objecthood) and how AI perceives them. The paper examines some of the most prominent AI paradigms dealing with object understanding and compares their methods of evaluating objecthood.\n",
      "\n",
      "3. \"Explicit Group Sparse Projection with Applications to Deep Learning and NMF\" by Riyasat Ohib, Nicolas Gillis, Niccolo Dalmasso, Sameena Shah, Vamsi Potluru, and Sergey Plis. This paper presents a new sparse projection method for a set of vectors that guarantees a desired average sparsity level measured leveraging the popular Hoyer measure. This projection is used for sparse dictionary learning and can be used to sparsify the columns of a matrix, which is used to compute sparse low-rank matrix approximations.\n",
      "\n",
      "4. \"Neural RF SLAM for unsupervised positioning and mapping with channel state information\" by S Kadambi, A Behboodi, JB Soriaga, M Welling, R Amiri, and S Yerramalli. This paper presents a neural network architecture for jointly learning user locations and environment mapping up to isometry, in an unsupervised way, from channel state information. The model is based on an encoder-decoder architecture and exploits multipath propagation to increase the precision of the system.\n",
      "\n",
      "5. \"LLMs in Animal-AI (LLM-AAI), a framework for conducting robust cognitive evaluations of the physical common-sense\". This paper introduces a framework called LLM-AAI, which integrates Large Language Models (LLMs) with the Animal-AI (AAI) 3D simulation environment. This framework combines Deep Reinforcement Learning (DRL), simulation, and generative AI to advance AI's understanding of the visual world and conduct robust cognitive evaluations of the physical common-sense.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '–ó–Ω–∞–π–¥–∏ 5 –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –Ω–∞—É–∫–æ–≤—ñ –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π –Ω–∞ —Ç–µ–º—É —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É: –Ω–∞–∑–≤—É, –∞–≤—Ç–æ—Ä—ñ–≤ —ñ –∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å.',\n",
       " 'output': 'Here are the 5 latest scientific publications on artificial intelligence:\\n\\n1. \"LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning\" by Hui Wang. This paper aims to develop a Gomoku AI system based on Large Language Models (LLMs), simulating the human learning process of playing chess. The system is designed to understand and apply Gomoku strategies through self-play and reinforcement learning.\\n\\n2. \"Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI\" by the University of Cambridge and the Technical University of Valencia. This paper aims to bridge the gap between cognitive science\\'s understanding of how humans and animals perceive objects (objecthood) and how AI perceives them. The paper examines some of the most prominent AI paradigms dealing with object understanding and compares their methods of evaluating objecthood.\\n\\n3. \"Explicit Group Sparse Projection with Applications to Deep Learning and NMF\" by Riyasat Ohib, Nicolas Gillis, Niccolo Dalmasso, Sameena Shah, Vamsi Potluru, and Sergey Plis. This paper presents a new sparse projection method for a set of vectors that guarantees a desired average sparsity level measured leveraging the popular Hoyer measure. This projection is used for sparse dictionary learning and can be used to sparsify the columns of a matrix, which is used to compute sparse low-rank matrix approximations.\\n\\n4. \"Neural RF SLAM for unsupervised positioning and mapping with channel state information\" by S Kadambi, A Behboodi, JB Soriaga, M Welling, R Amiri, and S Yerramalli. This paper presents a neural network architecture for jointly learning user locations and environment mapping up to isometry, in an unsupervised way, from channel state information. The model is based on an encoder-decoder architecture and exploits multipath propagation to increase the precision of the system.\\n\\n5. \"LLMs in Animal-AI (LLM-AAI), a framework for conducting robust cognitive evaluations of the physical common-sense\". This paper introduces a framework called LLM-AAI, which integrates Large Language Models (LLMs) with the Animal-AI (AAI) 3D simulation environment. This framework combines Deep Reinforcement Learning (DRL), simulation, and generative AI to advance AI\\'s understanding of the visual world and conduct robust cognitive evaluations of the physical common-sense.'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({'input': \"–ó–Ω–∞–π–¥–∏ 5 –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –Ω–∞—É–∫–æ–≤—ñ –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π –Ω–∞ —Ç–µ–º—É —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É: –Ω–∞–∑–≤—É, –∞–≤—Ç–æ—Ä—ñ–≤ —ñ –∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOqujC6qY_NY"
   },
   "source": [
    "\n",
    "\n",
    "### –ó–∞–≤–¥–∞–Ω–Ω—è 4: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞–≥–µ–Ω—Ç–∞-–ø–æ–º—ñ—á–Ω–∏–∫–∞ –¥–ª—è –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –±—ñ–∑–Ω–µ—Å-–∑–∞–¥–∞—á\n",
    "\n",
    "–°—Ç–≤–æ—Ä—ñ—Ç—å –∞–≥–µ–Ω—Ç–∞, —è–∫–∏–π –¥–æ–ø–æ–º–∞–≥–∞—î –≤–∏—Ä—ñ—à—É–≤–∞—Ç–∏ –∑–∞–¥–∞—á—ñ –±—ñ–∑–Ω–µ—Å-–∞–Ω–∞–ª—ñ—Ç–∏–∫–∏. –ê–≥–µ–Ω—Ç –º–∞—î –¥–æ–ø–æ–º–æ–≥—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—É —Å—Ç–≤–æ—Ä–∏—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Ä—ñ–∫ –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ —Ä—ñ–≤–µ–Ω—å —ñ–Ω—Ñ–ª—è—Ü—ñ—ó —ñ –ø–æ–≥–æ–¥–Ω—ñ —É–º–æ–≤–∏. –ê–≥–µ–Ω—Ç –º–∞—î –≤–º—ñ—Ç–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Python —ñ —Ö–æ–¥–∏—Ç–∏ –≤ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç –∞–±–∏ –æ—Ç—Ä–∏–º–∞—Ç–∏ –∞–∫—Ç—É–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ.\n",
    "\n",
    "**–ö—Ä–æ–∫–∏:**\n",
    "1. –ù–∞–ª–∞—à—Ç—É–π—Ç–µ –∞–≥–µ–Ω—Ç–∞, —è–∫–∏–π –ø—Ä–∞—Ü—é–≤–∞—Ç–∏–º–µ –∑ –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏, –∑–∞–¥–∞–Ω–∏–º–∏ —Ç–µ–∫—Å—Ç–æ–º. –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –ø–∏—à–µ\n",
    "\n",
    "```\n",
    "–ú–∏ –µ–∫—Å–ø–æ—Ä—Ç—É—î–º–æ –∞–ø–µ–ª—å—Å–∏–Ω–∏ –∑ –ë—Ä–∞–∑–∏–ª—ñ—ó. –í 2021 –µ–∫—Å–ø–æ—Ä—Ç—É–≤–∞–ª–∏ 200—Ç, –≤ 2022 - 190—Ç, –≤ 2023 - 210—Ç, –≤ 2024 —è–∫–∏–π —â–µ –Ω–µ –∑–∞–∫—ñ–Ω—á–∏–≤—Å—è - 220—Ç. –ó—Ä–æ–±–∏ –æ—Ü—ñ–Ω–∫—É —Å–∫—ñ–ª—å–∫–∏ –º–∏ –∑–º–æ–∂–µ–º–æ –µ–∫—Å–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –∞–ø–µ–ª—å—Å–∏–Ω—ñ–≤ –≤ 2025 –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ –ø–æ–≥–æ–¥–Ω—ñ —É–º–æ–≤–∏ –≤ –ë—Ä–∞–∑–∏–ª—ñ—ó —ñ –ø–æ–ø–∏—Ç –Ω–∞ –∞–ø–µ–ª—å—Å–∏–Ω–∏ –≤ —Å–≤—ñ—Ç—ñ –≤–∏—Ö–æ–¥—è—á–∏ –∑ –µ–∫–æ–Ω–æ–º—ñ—á–Ω–æ—ó —Å–∏—Ç—É–∞—Ü—ñ—ó.\n",
    "```\n",
    "\n",
    "2. –°—Ç–≤–æ—Ä—ñ—Ç—å –∑–∞–ø–∏—Ç –¥–æ –∞–≥–µ–Ω—Ç–∞, —â–æ –º—ñ—Å—Ç–∏—Ç—å —á—ñ—Ç–∫–µ –∑–∞–≤–¥–∞–Ω–Ω—è ‚Äì –≤–∏–¥–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±—ñ–∑–Ω–µ—Å –∞–Ω–∞–ª—ñ–∑—É –∞–±–æ –Ω–∞–ø–∏—Å–∞—Ç–∏, —â–æ –≤—ñ–Ω –Ω–µ –º–æ–∂–µ —Ü—å–æ–≥–æ –∑—Ä–æ–±–∏—Ç–∏ —ñ –∑–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ (–ø—Ä–æ—Å—Ç–æ –º–æ–∂–µ –±—É—Ç–∏ –≤—Å–µ –æ–¥–Ω–∏–º –ø–æ–≤—ñ–¥–æ–º–ª–ª–µ–Ω–Ω—è–º).\n",
    "\n",
    "3. –ó–∞–ø—É—Å—Ç—ñ—Ç—å –∞–≥–µ–Ω—Ç–∞ —ñ –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏. –©–æ –º–æ–∂–Ω–∞ –ø–æ–∫—Ä–∞—â–∏—Ç–∏?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "UK6KA9PyRS0L"
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.utilities import PythonREPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_repl = PythonREPL()\n",
    "python_tool = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you necessarily should print it out with `print(...)`. Otherwise you won't see the result! It's very important.\",\n",
    "    func=python_repl.run,\n",
    ")\n",
    "python_tool.name = \"Python Interpreter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(python_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThis is a complex question that requires multiple steps to answer. First, I need to find the expected inflation rate for 2025. Then, I need to find the expected weather conditions in Brazil for 2025. Finally, I need to analyze the global economic situation and its impact on the demand for oranges. Let's start with the inflation rate.\n",
      "\n",
      "Action: Search\n",
      "Action Input: \"expected inflation rate 2025\"\u001b[0m\u001b[36;1m\u001b[1;3m[{'title': 'Core inflation in February hits 2.8%, higher than expected; spending increases 0.4%', 'link': 'https://www.cnbc.com/2025/03/28/pce-inflation-february-2025-.html', 'source': 'CNBC', 'source_logo': 'https://serpapi.com/searches/67e9b16079d2237c5f7c2fe0/images/58265cdb1ca2a3a6a1f94883df3f021fcc749eb18f194027558a582215430b1d.png', 'date': '2 days ago', 'thumbnail': 'https://serpapi.com/searches/67e9b16079d2237c5f7c2fe0/images/58265cdb1ca2a3a6b99a34e81a8fabc8a0e68d3a75904cf2.jpeg'}, {'title': 'Moderate US consumer spending, increasing prices raise specter of stagflation', 'link': 'https://www.reuters.com/markets/us/us-consumer-spending-rises-february-core-inflation-firmer-2025-03-28/', 'source': 'Reuters', 'source_logo': 'https://serpapi.com/searches/67e9b16079d2237c5f7c2fe0/images/58265cdb1ca2a3a6a346db3259d7e5e255c7ddc43c6a6115263765f1515f251f.png', 'date': '2 days ago', 'thumbnail': 'https://serpapi.com/searches/67e9b16079d2237c5f7c2fe0/images/58265cdb1ca2a3a6283d5fbceef91d07d680e7d95f0302f4.jpeg'}, {'title': 'Core inflation ticks up, with PCE report showing prices rising faster than economists had forecast', 'link': 'https://www.cbsnews.com/news/pce-report-data-inflation-higher-than-expected-february-2025/', 'source': 'CBS News', 'source_logo': 'https://serpapi.com/searches/67e9b16079d2237c5f7c2fe0/images/58265cdb1ca2a3a6bbdf320b6bcb3fef1e48e814a9e7a300bcf0e70b4558ac9b.png', 'date': '2 days ago', 'thumbnail': 'https://serpapi.com/searches/67e9b16079d2237c5f7c2fe0/images/58265cdb1ca2a3a6af7f6c03d23ca64d0d16e25aa03c52f8.jpeg'}, {'title': 'Some economists think U.S. inflation is likely to rise in 2025', 'link': 'https://www.cbsnews.com/news/inflation-trump-tariffs-economists-forecast-2025/', 'source': 'CBS News', 'source_logo': 'https://serpapi.com/searches/67e9b16079d2237c5f7c2fe0/images/58265cdb1ca2a3a6e423cb44bc14c8d64d7f76063ae6f22df7b8e9976d2eed9e.png', 'date': '3 weeks ago', 'thumbnail': 'https://serpapi.com/searches/67e9b16079d2237c5f7c2fe0/images/58265cdb1ca2a3a6c81d3e4e817faeddcf0e819016211723.jpeg'}]\u001b[0m\u001b[32;1m\u001b[1;3mThe search results do not provide a specific expected inflation rate for 2025. However, they suggest that inflation is expected to rise. I will need to make an assumption for the inflation rate. Let's assume a 3% inflation rate for 2025. Now, let's move on to the expected weather conditions in Brazil for 2025.\n",
      "\n",
      "Action: Search\n",
      "Action Input: \"expected weather conditions in Brazil 2025\"\u001b[0m\u001b[36;1m\u001b[1;3m[' entity_type: related_questions.', 'Expect daytime maximum temperatures of 24¬∞C in Brazil in July based on long-term weather averages. There are 6 hours of sunshine per day on average.', 'During August, expect a variety of temperatures with highs around 26¬∞ and lows near 18¬∞, suitable for diverse activities and preferences.', 'Expect daytime maximum temperatures of 26¬∞C in Brazil in May based on long-term weather averages. There are 6 hours of sunshine per day on average.', 'The temperatures in Brazil in June are comfortable with low of 57¬∞F and and high up to 71¬∞F. You can expect about 3 to 8 days of rain in Brazil during the ...', 'Temperatures as high as 30¬∞C to 44¬∞C (86¬∞F to 111¬∞F) are well above the mid-to-late February normal, with some locations in Brazil experiencing ...', 'During July, expect a variety of temperatures with highs around 25¬∞ and lows near 18¬∞, suitable for diverse activities and preferences. How much rainfall should ...', 'Get the monthly weather forecast for 25, S√£o Paulo, Brazil, including daily high/low, historical averages, to help you plan ahead.', 'The winters are quite mild, with night temperatures dropping to 19¬∞C, and daytime temperatures climbing to 30¬∞C. Note that most houses and apartments are built ...', 'The temperatures in Brazil in April are comfortable with low of 62¬∞F and and high up to 75¬∞F. You can expect about 3 to 8 days of rain in Brazil during the ...']\u001b[0m\u001b[32;1m\u001b[1;3mThe search results do not provide specific expected weather conditions for Brazil in 2025. However, they provide some general trends based on long-term averages. It seems that temperatures are generally comfortable for orange cultivation, with daytime maximum temperatures ranging from 24¬∞C to 30¬∞C. Rainfall seems to vary, but there are no indications of extreme weather conditions that could severely impact orange production. Now, let's move on to the global economic situation and its impact on the demand for oranges.\n",
      "\n",
      "Action: Search\n",
      "Action Input: \"global economic outlook 2025\"\u001b[0m\u001b[36;1m\u001b[1;3m[{'title': 'Economic conditions outlook, March 2025', 'link': 'https://www.mckinsey.com/capabilities/strategy-and-corporate-finance/our-insights/economic-conditions-outlook', 'source': 'McKinsey & Company', 'source_logo': 'https://serpapi.com/searches/67e9b16baa6533b80185864c/images/bd429949e20c693f6e7da4d1a710070cffc0989c2c447adef41de51dfae97701.jpeg', 'date': '2 days ago', 'thumbnail': 'https://serpapi.com/searches/67e9b16baa6533b80185864c/images/bd429949e20c693f7e9462a84bff5789ba9c70a21215a1e3.jpeg'}, {'title': 'Global Economic Outlook - March 2025', 'link': 'https://www.fitchratings.com/research/sovereigns/global-economic-outlook-march-2025-18-03-2025', 'source': 'Fitch Ratings', 'source_logo': 'https://serpapi.com/searches/67e9b16baa6533b80185864c/images/bd429949e20c693f893677c324a8bb4cec44b87763beb0a7412a2a78aedb2974.png', 'date': '2 weeks ago'}, {'title': 'Global economic outlook uncertain as growth slows, inflationary pressures persist and trade policies cloud outlook', 'link': 'https://www.oecd.org/en/about/news/press-releases/2025/03/global-economic-outlook-uncertain-as-growth-slows-inflationary-pressures-persist-and-trade-policies-cloud-outlook.html', 'source': 'OECD', 'source_logo': 'https://serpapi.com/searches/67e9b16baa6533b80185864c/images/bd429949e20c693faa936bbfe1fe3451d988f628b3e3338252fea7680885e016.png', 'date': '2 weeks ago', 'thumbnail': 'https://serpapi.com/searches/67e9b16baa6533b80185864c/images/bd429949e20c693f63e7c922cadf73d1a208d77f6496242a.jpeg'}]\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThe search results suggest that the global economic outlook for 2025 is uncertain, with slowing growth, inflationary pressures, and trade policies potentially impacting the demand for oranges. Given this uncertainty, it's difficult to make a precise prediction for orange exports in 2025. However, I can use the past export data to make a simple linear regression prediction for 2025, assuming that the trends observed in the past will continue into the future.\n",
      "\n",
      "Action: Python Interpreter\n",
      "Action Input: \n",
      "\"\"\"\n",
      "import numpy as np\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "# Years and corresponding export quantities\n",
      "years = np.array([2021, 2022, 2023, 2024]).reshape((-1, 1))\n",
      "quantities = np.array([200, 190, 210, 220])\n",
      "\n",
      "# Create and fit the model\n",
      "model = LinearRegression().fit(years, quantities)\n",
      "\n",
      "# Predict the export quantity for 2025\n",
      "prediction = model.predict(np.array([[2025]]))\n",
      "print(prediction)\n",
      "\"\"\"\u001b[0m\u001b[33;1m\u001b[1;3m[225.]\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mThe linear regression model predicts that we will export approximately 225 tons of oranges in 2025, assuming that the trends observed in the past continue into the future. However, this prediction does not take into account the expected inflation rate, the expected weather conditions in Brazil, or the global economic situation. Given the expected increase in inflation, the generally favorable weather conditions in Brazil, and the uncertain global economic outlook, the actual export quantity could be higher or lower than this prediction.\n",
      "\n",
      "Final Answer: Based on the data provided and the trends observed in the past, it is estimated that approximately 225 tons of oranges will be exported in 2025. However, this is a rough estimate and the actual quantity could be influenced by various factors such as the inflation rate, weather conditions in Brazil, and the global economic situation. It is recommended to regularly review and update this forecast as new data becomes available.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '\\n–î–æ–ø–æ–º–æ–∂–∏ –º–µ–Ω—ñ –ø—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂—ñ–≤ –Ω–∞ 2025 —Ä—ñ–∫, –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ –æ—á—ñ–∫—É–≤–∞–Ω–∏–π —Ä—ñ–≤–µ–Ω—å —ñ–Ω—Ñ–ª—è—Ü—ñ—ó —Ç–∞ –ø–æ–≥–æ–¥–Ω—ñ —É–º–æ–≤–∏.\\n–ú–∏ –µ–∫—Å–ø–æ—Ä—Ç—É—î–º–æ –∞–ø–µ–ª—å—Å–∏–Ω–∏ –∑ –ë—Ä–∞–∑–∏–ª—ñ—ó. –í 2021 –µ–∫—Å–ø–æ—Ä—Ç—É–≤–∞–ª–∏ 200—Ç, –≤ 2022 - 190—Ç, –≤ 2023 - 210—Ç, –≤ 2024 —è–∫–∏–π —â–µ –Ω–µ –∑–∞–∫—ñ–Ω—á–∏–≤—Å—è - 220—Ç. –ó—Ä–æ–±–∏ –æ—Ü—ñ–Ω–∫—É —Å–∫—ñ–ª—å–∫–∏ –º–∏ –∑–º–æ–∂–µ–º–æ –µ–∫—Å–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –∞–ø–µ–ª—å—Å–∏–Ω—ñ–≤ –≤ 2025 –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ –ø–æ–≥–æ–¥–Ω—ñ —É–º–æ–≤–∏ –≤ –ë—Ä–∞–∑–∏–ª—ñ—ó —ñ –ø–æ–ø–∏—Ç –Ω–∞ –∞–ø–µ–ª—å—Å–∏–Ω–∏ –≤ —Å–≤—ñ—Ç—ñ –≤–∏—Ö–æ–¥—è—á–∏ –∑ –µ–∫–æ–Ω–æ–º—ñ—á–Ω–æ—ó —Å–∏—Ç—É–∞—Ü—ñ—ó.\\n', 'output': 'Based on the data provided and the trends observed in the past, it is estimated that approximately 225 tons of oranges will be exported in 2025. However, this is a rough estimate and the actual quantity could be influenced by various factors such as the inflation rate, weather conditions in Brazil, and the global economic situation. It is recommended to regularly review and update this forecast as new data becomes available.'}\n"
     ]
    }
   ],
   "source": [
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "request = \"\"\"\n",
    "–î–æ–ø–æ–º–æ–∂–∏ –º–µ–Ω—ñ –ø—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂—ñ–≤ –Ω–∞ 2025 —Ä—ñ–∫, –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ –æ—á—ñ–∫—É–≤–∞–Ω–∏–π —Ä—ñ–≤–µ–Ω—å —ñ–Ω—Ñ–ª—è—Ü—ñ—ó —Ç–∞ –ø–æ–≥–æ–¥–Ω—ñ —É–º–æ–≤–∏.\n",
    "–ú–∏ –µ–∫—Å–ø–æ—Ä—Ç—É—î–º–æ –∞–ø–µ–ª—å—Å–∏–Ω–∏ –∑ –ë—Ä–∞–∑–∏–ª—ñ—ó. –í 2021 –µ–∫—Å–ø–æ—Ä—Ç—É–≤–∞–ª–∏ 200—Ç, –≤ 2022 - 190—Ç, –≤ 2023 - 210—Ç, –≤ 2024 —è–∫–∏–π —â–µ –Ω–µ –∑–∞–∫—ñ–Ω—á–∏–≤—Å—è - 220—Ç. –ó—Ä–æ–±–∏ –æ—Ü—ñ–Ω–∫—É —Å–∫—ñ–ª—å–∫–∏ –º–∏ –∑–º–æ–∂–µ–º–æ –µ–∫—Å–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –∞–ø–µ–ª—å—Å–∏–Ω—ñ–≤ –≤ 2025 –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ –ø–æ–≥–æ–¥–Ω—ñ —É–º–æ–≤–∏ –≤ –ë—Ä–∞–∑–∏–ª—ñ—ó —ñ –ø–æ–ø–∏—Ç –Ω–∞ –∞–ø–µ–ª—å—Å–∏–Ω–∏ –≤ —Å–≤—ñ—Ç—ñ –≤–∏—Ö–æ–¥—è—á–∏ –∑ –µ–∫–æ–Ω–æ–º—ñ—á–Ω–æ—ó —Å–∏—Ç—É–∞—Ü—ñ—ó.\n",
    "\"\"\"\n",
    "response = agent_executor.invoke({'input': request})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–∏—Å–Ω–æ–≤–æ–∫ - –ù–µ –ø–æ–≥–∞–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
